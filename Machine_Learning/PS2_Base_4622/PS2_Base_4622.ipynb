{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 - Decision Trees, Model Selection, and Ensemble Methods\n",
    "## CSCI 4622 - Spring 2022\n",
    "***\n",
    "**Name**: $<John Wilson>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11:59PM on February 23rd**.\n",
    "\n",
    "Submit only this Jupyter notebook to Canvas with the name format `PS2_<yourname>.ipynb`. Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted.\n",
    "The only exception to this rule is that you may copy code directly from your own solution to homework 1.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0c6d8e8d7f7dc1171c03c5dccc47a0c",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Overview \n",
    "\n",
    "Your task for this homework is to build a decision tree classifier from scratch. Of course, we provide some initial classes\n",
    "that you'll be editing. Since last two problems will use the scikit-learn's DecisionTreeClassifier, your solution\n",
    "does not have to be efficient as long as it passes the sanity checks in a reasonable time (typically less than ~1min).\n",
    "\n",
    "We will run a small comparison between our implementation and Scikit's in Problem 2 to make sure we didn't miss anything.\n",
    "\n",
    "The third part will introduce k-fold cross validation to find out how deep is the best decision tree classifier. The last problem\n",
    "requires a _weak learner_, so we'll use a decision tree that yields lower performance. But with _Ensemble Methods_, we will be able to improve the performance by aggregating predictions from multiple weak learners.\n",
    "For the ensemble methods, we'll explore bagging, Random Forest, and boosting (AdaBoost).\n",
    "\n",
    "Any Machine Learning interview will almost certainly have a question or two about decision trees and how they're trained.\n",
    "So understanding the code and trying to implement everything on your own will be the best way to prepare for such interviews.\n",
    "\n",
    "Also remember, if your code is correct then the sanity checks should pass without any major issue.\n",
    "But if the sanity checks pass that does not necessarily imply your code is 100% correct.\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b665692ac38f1b9fa4c6f3112b828d5",
     "grade": false,
     "grade_id": "imports_p1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tests\n",
    "import data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74f3a1c373fd347807123286cc8ab4ec",
     "grade": false,
     "grade_id": "datatable",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 1 - Decision Trees [40 points]\n",
    "***\n",
    "The goal of this problem is to implement the core elements of the Decision Tree classifier.\n",
    "We do not expect a highly efficient implementation of the functions since the ensemble methods will\n",
    "use the implementation from scikit-learn.\n",
    "\n",
    "We'll be testing our implementation on the same dataset we used for Naive Bayes.\n",
    "\n",
    "|Age|Salary|Colorado Resident| Has Siblings | College degree|\n",
    "|:------:|:-----------:| :----------:| :----------:|--:|\n",
    "| 37 | 44,000 | Yes | No  | Yes|\n",
    "| 61 | 52,000 | Yes | No  | No |\n",
    "| 23 | 44,000 | No  | No  | Yes|\n",
    "| 39 | 38,000 | No  | Yes | Yes|\n",
    "| 48 | 49,000 | No  | No  | Yes|\n",
    "| 57 | 92,000 | No  | Yes | No |\n",
    "| 38 | 41,000 | No  | Yes | Yes|\n",
    "| 27 | 35,000 | Yes | No  | No |\n",
    "| 23 | 26,000 | Yes | No  | No |\n",
    "| 38 | 45,000 | No  | No  | No |\n",
    "| 32 | 50,000 | No  | No  | Yes|\n",
    "| 25 | 52,000 | Yes | No  | Yes|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1aee060e8d9457b3ed79edc1c7fa0725",
     "grade": false,
     "grade_id": "cell-77099ab2229fd7e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = np.array([\n",
    "    [37, 44000, 1, 0],\n",
    "    [61, 52000, 1, 0],\n",
    "    [23, 44000, 0, 0],\n",
    "    [39, 38000, 0, 1],\n",
    "    [48, 49000, 0, 0],\n",
    "    [57, 92000, 0, 1],\n",
    "    [38, 41000, 0, 1],\n",
    "    [27, 35000, 1, 0],\n",
    "    [23, 26000, 1, 0],\n",
    "    [38, 45000, 0, 0],\n",
    "    [32, 50000, 0, 0],\n",
    "    [25, 52000, 1, 0]\n",
    "])\n",
    "labels = np.array([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3be09e728edcd4e549c124d66a29eea",
     "grade": false,
     "grade_id": "q11_12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each leaf node (terminal node) in a decision tree has a label value assigned to it. The same label will be assigned\n",
    "to all samples that reach the leaf node.\n",
    "- 1.1 [2 pts] What is the best accuracy for a baseline classifier that predicts one label for all rows on the dataset above?\n",
    "which label should it predict?\n",
    "- 1.2 [5 pts] Complete `compute_label` to return the label that should be assigned to the leaf node based on training labels in `y`. If more than one label are possible, choose the one with the lowest value (e.g, if both `0` and `1` are possible, choose `0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ece7cf4df488ac69fd5a2ae22baadcb5",
     "grade": true,
     "grade_id": "a11",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "% Write-up for 1.1 <br>\n",
    "%BEGIN\n",
    "\n",
    "%7/12 or 0.583% accuracy if the label 1 is predicted \n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f62d0c179369de22fcec6007dc8ff453",
     "grade": true,
     "grade_id": "a12",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LeafNode:\n",
    "    def __init__(self, y):\n",
    "        \"\"\"\n",
    "        :param y: 1-d array containing labels, of shape (num_points,)\n",
    "        \"\"\"\n",
    "        self.label = self.compute_label(y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (type(self) == type(other)) and (self.label == other.label)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_label(y):\n",
    "        \"\"\"\n",
    "        return the label that yields best performance if predicted of all instances in y\n",
    "        :param y:  1-d array containing labels\n",
    "        :return: single label, integer\n",
    "        \"\"\"\n",
    "        node_label = None\n",
    "        #Workspace 1.2\n",
    "        #TODO: Return the label that should be assigned to the leaf node\n",
    "        #In case of multiple possible labels, choose the one with the lowest value\n",
    "        #Make no assumptions about the number of class labels\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        label_arr = []\n",
    "        for i in y:\n",
    "            if i in label_arr:\n",
    "                continue\n",
    "            else:\n",
    "                label_arr.append(i)\n",
    "        accuracy = 0\n",
    "        for i in label_arr:\n",
    "            num_total = 0\n",
    "            num_correct = 0\n",
    "            for j in range(len(y)):\n",
    "                if y[j] == i:\n",
    "                    num_correct += 1\n",
    "                    num_total += 1\n",
    "                else:\n",
    "                    num_total += 1\n",
    "            if (num_correct / num_total) > accuracy:\n",
    "                accuracy = (num_correct / num_total)\n",
    "                node_label = i\n",
    "            if (num_correct / num_total) == accuracy and accuracy != 0 and node_label != None:\n",
    "                if node_label > i:\n",
    "                    nodel_label = i\n",
    "        #END\n",
    "        return node_label\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        return the label for one obervation x\n",
    "        :param x: one sample, of shape (num_features)\n",
    "        :return: label, integer\n",
    "        \"\"\"\n",
    "        return self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.2: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_leaf(LeafNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "267eb763503c9fe383d2745660faed29",
     "grade": false,
     "grade_id": "q13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### The tree also contains _parent nodes_. They can either be parents of: leaf nodes, parent nodes, or a combination of the two.\n",
    "Each parent node has a left and a right child. A parent node is used when we can reduce the impurity of the labels by splitting\n",
    "the training instances based on a certain threshold.\n",
    "\n",
    "First, we'll need to choose an impurity measure. For classification, there are two mainstream measures: _gini index_ and _entropy_. We'll be using the latter for our implementation.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Gini}(y) = 1 - \\sum_{c}  (p_c)^2 \\text{  and  Entropy}(y) = -\\sum_{c}  p_c . \\log p_c ,\n",
    "\\end{align}\n",
    "\n",
    "where $p_c$ is the probability of occurrence (ratio)  of class $c$ among the labels in $y$\n",
    "\n",
    "- 1.3 [5 pts] Complete the function `entropy` that returns the entropy measure of labels in `y`.\n",
    "\n",
    "_Hint: for the log function, use `np.log` and the convention `0 * log(0) = 0`. Make sure you handle multi-class labels (not just binary)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6928fb85ba1ce6f443e68613dce870c",
     "grade": true,
     "grade_id": "a13",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    :param y: 1-d array contains labels, of shape (num_points,)\n",
    "    :return: float, entropy measure of the labels\n",
    "    \"\"\"\n",
    "    entropy_value = 0\n",
    "    # Workspace 1.3\n",
    "    #TODO: Compute the entropy of the labels\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    label_arr = []\n",
    "    label_prob = []\n",
    "    for i in y:\n",
    "        if i in label_arr:\n",
    "            continue\n",
    "        else:\n",
    "            label_arr.append(i)\n",
    "    for i in label_arr:\n",
    "        num_label = 0\n",
    "        for j in y:\n",
    "            if j == i:\n",
    "                num_label += 1\n",
    "        label_prob.append(num_label / len(y))\n",
    "    log_arr = np.log(label_prob)\n",
    "    for i in range (len(label_prob)):\n",
    "        entropy_value += (label_prob[i] * log_arr[i])\n",
    "    entropy_value = entropy_value * -1\n",
    "    #END\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.3: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_entropy(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b019643aa64c03373356d0ccf6f4d12",
     "grade": false,
     "grade_id": "q14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we're at a parent node, we decide to partition our label instances in $S$ to two parts indexed by $P_1$ and $P_2$,\n",
    "and we want to compute how much this split reduces the impurity.\n",
    "\n",
    "Using the impurity measure $\\mathcal{M}$, this impurity reduction is computed as follows:\n",
    "\\begin{align}\n",
    "\\text{Reduction}(S, {P_1, P_2}) = \\mathcal{M}(S) - \\big[\n",
    "    \\frac{|P_1|}{|S|} .\\mathcal{M}(S[P_1]) + \\frac{|P_2|}{|S|}.\\mathcal{M}(S[P_2])\n",
    "    \\big],\n",
    "\\end{align}\n",
    "\n",
    "where $|A|$ denotes the size of the set $A$.\n",
    "\n",
    "The main questions will be based on the entropy measure, in which case the `Reduction` is also called _information gain_\n",
    "(reducing the entropy implies that the partitioning decision variable and the labels have a higher mutual information).\n",
    "\n",
    "-  1.4 [5 pts] Complete the `impurity_reduction` function to return the impurity reduction of the split using the provided measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35bb9913b0764fe97e30e6fde5a0a0f6",
     "grade": true,
     "grade_id": "a14",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def impurity_reduction(y, left_indices, right_indices, impurity_measure=entropy):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param left_indices: the indices of the elements of y that belong to the left child\n",
    "    :param right_indices: the indices of the elements of y that belong to the right child\n",
    "    :param impurity_measure: function that takes 1d-array of labels and returns the impurity measure, defaults to entropy\n",
    "    :return: impurity reduction of the split\n",
    "    \"\"\"\n",
    "    impurity_reduce = 0\n",
    "    # Workspace 1.4\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    impurity_reduce = impurity_measure(y) - (((len(left_indices) / len(y)) * impurity_measure(y[left_indices])) + ((len(right_indices) / len(y)) * impurity_measure(y[right_indices])))\n",
    "    #END\n",
    "    return impurity_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.4: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_information_gain(impurity_reduction, entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b3c16847773c2a17c5f7f96a057dece",
     "grade": false,
     "grade_id": "q15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll use `best_partition` to look up for the feature and threshold that yields the partition with the best impurity reduction.\n",
    "\n",
    "For each feature:\n",
    " - Compute all possible thresholds (use `split_values`)\n",
    " - For each threshold:\n",
    "    - Split to `(left_indices, right_indices)` based on the threshold\n",
    "    - Compute the impurity reduction of the split\n",
    "\n",
    "The function then returns the feature and the threshold that yield the best impurity reduction (and the reduction value)\n",
    "\n",
    " - 1.5 [7 pts] Complete `best_partition`.\n",
    " \n",
    " _Hint: `split_values` is provided as a helper function. It takes the feature column and returns\n",
    "the set of thresholds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9885dfd2c0e096009f94210889f8bd3e",
     "grade": true,
     "grade_id": "a15",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_values(feature_values):\n",
    "    \"\"\"\n",
    "    Helper function to return the split values. if feature consists of the values f1 < f2 < f3 then\n",
    "    this returns [(f2 + f1)/2, (f3 + f2)/2]\n",
    "    :param feature_values: 1-d array of shape (num_points)\n",
    "    :return: array of shape (max(m-1, 1),) where m is the number of unique values in feature_values\n",
    "    \"\"\"\n",
    "    unique_values = np.unique(feature_values)\n",
    "    if unique_values.shape[0] == 1:\n",
    "        return unique_values\n",
    "    return (unique_values[1:] + unique_values[:-1]) / 2\n",
    "\n",
    "\n",
    "def best_partition(X, y, impurity_measure=entropy):\n",
    "    \"\"\"\n",
    "    :param X: features array, shape (num_samples, num_features)\n",
    "    :param y: labels of instances in X, shape (num_samples)\n",
    "    :param impurity_measure: function that takes 1d-array of labels and returns the impurity measure\n",
    "    :return: Return the best value and its corresponding threshold by splitting based on the different features.\n",
    "    \"\"\"\n",
    "\n",
    "    best_feature, best_threshold, best_reduction = 0, 0, -np.inf\n",
    "\n",
    "    #Workspace 1.5\n",
    "    #TODO: Complete the function as detailed in the question and return description\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    x_Array = np.array(X)\n",
    "    for i in range (np.size(x_Array, 1)):\n",
    "        column = x_Array[:, i]\n",
    "        threshold = split_values(column)\n",
    "        for j in threshold:\n",
    "            left_indices = []\n",
    "            right_indices = []\n",
    "            for k in range (len(column)):\n",
    "                if column[k] > j:\n",
    "                    left_indices.append(k)\n",
    "                else:\n",
    "                    right_indices.append(k)\n",
    "            impurity_reduce = impurity_measure(y) - (((len(left_indices) / len(column)) * impurity_measure(y[left_indices])) + ((len(right_indices) / len(column)) * impurity_measure(y[right_indices])))\n",
    "            #print(impurity_reduce)\n",
    "            if impurity_reduce > best_reduction:\n",
    "                best_reduction = impurity_reduce\n",
    "                best_threshold = j\n",
    "                best_feature = i\n",
    "    #END> \n",
    "    return best_feature, best_threshold, best_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.5: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# If you chose to not use split_values, then this test will likely fail\n",
    "tests.test_best_partition(best_partition, entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62c52953bd532bb6a14a4ed3b1dc2de6",
     "grade": false,
     "grade_id": "parent_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We provide the implementation of the parent node below. Note that the `left_child` will take instance for which\n",
    "`feature_id` value is < `feature_threshold`. We should construct our decision tree as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ParentNode:\n",
    "\n",
    "    def __init__(self, feature_id, feature_threshold, left_child, right_child):\n",
    "        \"\"\"\n",
    "        Initialize a parent node.\n",
    "        :param feature_id: the feature index on which the splitting will be done\n",
    "        :param feature_threshold: the feature threshold. Left child takes item with features[features_id] < threshold\n",
    "        :param left_child: left child node\n",
    "        :param right_child: right child node\n",
    "        \"\"\"\n",
    "        self.feature_id = feature_id\n",
    "        self.threshold = feature_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the label of row x. If we're a leaf node, return the value of the leaf. Otherwise, call predict\n",
    "        of the left/right child (depending on x[feature_index).\n",
    "        This will be called by DecisionTree.predict\n",
    "        :param x: 1-d array of shape (num_features)\n",
    "        :return: integer, the label for x\n",
    "        \"\"\"\n",
    "        if x[self.feature_id] < self.threshold:\n",
    "            label = self.left_child.predict(x)\n",
    "        else:\n",
    "            label = self.right_child.predict(x)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8977aac3ac2162c986ea0f2e77d5e233",
     "grade": false,
     "grade_id": "q16_17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we tackle the core of a decision tree. The tree is built in a recursive way. The recursion in `DecisionTree.build` works as follows:\n",
    "- Parameters: `min_samples_split`, `impurity_measure`\n",
    "- Inputs: `features`, `labels`, `depth`\n",
    "- Base case of the recursion, return a leaf node if either:\n",
    "    - `depth` is 0\n",
    "    - `labels` contains less than `min_samples_split` elements\n",
    "    - There is no impurity reduction (reduction<=0 for all splits)\n",
    "- Recursion (there is a split with impurity reduction > 0):\n",
    "    - create the left and right child nodes with `depth - 1`\n",
    "    - return the parent node\n",
    "\n",
    "The left child node will contain instances for which the feature with index `best_feature` is strictly lower than\n",
    "`best_threshold` of the partition. The right child takes the remaining instances.\n",
    "\n",
    "- 1.6 [8 pts] Complete `build` method of `DecisionTree`\n",
    "- 1.7 [2 pts] Complete the `score` method that returns the accuracy on the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef5b665f05e3dd92b435c4b70d2872b0",
     "grade": true,
     "grade_id": "a16_17",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=-1, min_samples_split=2, impurity_measure=entropy):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree\n",
    "        :param max_depth: maximum depth of the tree\n",
    "        :param min_samples_split: minimum number of samples required for a split\n",
    "        :param impurity_measure: impurity measure function to use for best_partition, default to entropy\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.impurity_measure = impurity_measure\n",
    "        self.root = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def build(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        Recursive method used to build the decision tree nodes\n",
    "        :param X: data that are used to build the tree, of shape (num_samples, num_features)\n",
    "        :param y: labels of the samples in features, of shape (num_samples)\n",
    "        :param depth: depth of the tree to create\n",
    "        :return: the root node of the tree\n",
    "        \"\"\"\n",
    "       \n",
    "       \n",
    "        if depth == 0 or len(y) < self.min_samples_split:\n",
    "            # we reached the maximum depth or we don't have more than the minimum number of samples in the leaf\n",
    "            return LeafNode(y)\n",
    "        else:\n",
    "            # Get the feature, threshold and information_gain of the best split\n",
    "            feature_id, threshold, gain = best_partition(X, y, self.impurity_measure)\n",
    "            # gain = 0 occurs when the labels have the same distribution in the child nodes\n",
    "            # which means that the entropy of the children is the same as the parent's\n",
    "            if gain > 0:\n",
    "                # Workspace 1.6\n",
    "                # TODO: create the left and right child nodes with depth - 1, return the parent node\n",
    "                #BEGIN\n",
    "                x_Array = np.array(X)\n",
    "                column = x_Array[:, feature_id]\n",
    "                left_indices = []\n",
    "                right_indices = []\n",
    "                for j in range (len(column)):\n",
    "                    if column[j] < threshold:\n",
    "                        left_indices.append(j)\n",
    "                    else:\n",
    "                        right_indices.append(j)\n",
    "                left_child = self.build(X[left_indices], y[left_indices], (depth-1))\n",
    "                right_child = self.build(X[right_indices], y[right_indices], (depth-1))\n",
    "                return ParentNode(feature_id, threshold, left_child, right_child)\n",
    "                #END\n",
    "            else:\n",
    "                # We reach here if information_gain <= 0\n",
    "                return LeafNode(y)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: Training samples\n",
    "        :param y: training labels\n",
    "        :return: trained classifier\n",
    "        \"\"\"\n",
    "        self.num_features = X.shape[1]\n",
    "        self.root = self.build(X, y, self.max_depth)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Loops through rows of X and predicts the labels one row at a time\n",
    "        \"\"\"\n",
    "        y_hat = np.zeros((X.shape[0],), int)\n",
    "        for i in range(X.shape[0]):\n",
    "            y_hat[i] = self.root.predict(X[i])\n",
    "        return y_hat\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        :param X: Test samples, shape (num_points, num_features)\n",
    "        :param y: true labels for X, shape (num_points,)\n",
    "        :return: mean accuracy\n",
    "        \"\"\"\n",
    "        accuracy = 0\n",
    "        # Workspace 1.7\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        y_hat = self.predict(X)\n",
    "        for i in range(len(y)):\n",
    "            if y_hat[i] == y[i]:\n",
    "                num_correct += 1\n",
    "                num_total += 1\n",
    "            else:\n",
    "                num_total += 1\n",
    "        accuracy = num_correct / num_total       \n",
    "        #END\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.6: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# If you chose to not use split_values, then this test will likely fail\n",
    "tests.test_tree_build(DecisionTree, entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd5a3c587c8ec35f8f9c702f84d29933",
     "grade": false,
     "grade_id": "q18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 1.8 [2 pts] We want to compare our `DecisionTree(max_depth=3, min_samples_split=2` to our NaiveBayes.\n",
    "What's the accuracy we achieve on the training data using the tree? ( we train and evaluate using `(features, labels)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4015467f2f89548d668cbd7393e9a4cf",
     "grade": true,
     "grade_id": "a18",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTree: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Workspace 1.8\n",
    "#BEGIN \n",
    "# code here\n",
    "#END\n",
    "tree = DecisionTree(max_depth=3, min_samples_split=2)\n",
    "tree.fit(features, labels)\n",
    "accuracy = tree.score(features, labels)\n",
    "print(\"Accuracy of DecisionTree:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38756b9794ee2734fba840eeca4da2ab",
     "grade": false,
     "grade_id": "q19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1.9 [2 pts] Using `min_samples_split=2`, what is the minimum depth so that our `DecisionTree` fits perfectly our\n",
    "training data `(labels, features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a67bc7261215441b222be45948125d92",
     "grade": true,
     "grade_id": "a19",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree of Depth n = 5 with accuracy =  1.0 has minimum depth for a perfect fit\n",
      "DecisionTree of Depth n = 4 with accuracy =  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Workspace 1.9\n",
    "# To show that the minimum required depth is n, you can provide the accuracy for depth = (n-1) and depth = n\n",
    "#BEGIN \n",
    "# code here\n",
    "n = 5\n",
    "tree2 = DecisionTree(max_depth=n, min_samples_split=2)\n",
    "tree2.fit(features, labels)\n",
    "accuracy = tree2.score(features, labels)\n",
    "print(\"DecisionTree of Depth n =\",n,\"with accuracy = \",accuracy,\"has minimum depth for a perfect fit\")\n",
    "n -= 1\n",
    "tree3 = DecisionTree(max_depth=n, min_samples_split=2)\n",
    "tree3.fit(features, labels)\n",
    "accuracy = tree3.score(features, labels)\n",
    "print(\"DecisionTree of Depth n =\",n,\"with accuracy = \",accuracy)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b58a4d2d50a836f50348c8b3344dddb5",
     "grade": false,
     "grade_id": "q110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We provide an example below to display the structure of a decision tree.\n",
    "- 1.10 (2pts) Edit it to show the tree for the required minimum depth found in 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ┌│label: 0\n",
      "       ┌|salary  │┘\n",
      "       │|36500.00│┐\n",
      "       │          │       ┌│label: 1\n",
      "       │          └|age  │┘\n",
      "       │           |37.50│┐\n",
      "       │                  └│label: 1\n",
      "|age  │┘\n",
      "|52.50│┐\n",
      "       └│label: 0\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(max_depth=3, min_samples_split=2).fit(features, labels)\n",
    "tests.print_tree(tree, [\"age\", \"salary\", \"resident\", \"siblings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d30ddb7d50cc5b1bed552b1e0dd66de7",
     "grade": true,
     "grade_id": "a110",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ┌│label: 0\n",
      "       ┌|salary  │┘\n",
      "       │|36500.00│┐\n",
      "       │          │       ┌│label: 1\n",
      "       │          └|age  │┘\n",
      "       │           |37.50│┐\n",
      "       │                  │                  ┌│label: 1\n",
      "       │                  │       ┌|salary  │┘\n",
      "       │                  │       │|43000.00│┐\n",
      "       │                  │       │          └│label: 0\n",
      "       │                  └|age  │┘\n",
      "       │                   |38.50│┐\n",
      "       │                          └│label: 1\n",
      "|age  │┘\n",
      "|52.50│┐\n",
      "       └│label: 0\n"
     ]
    }
   ],
   "source": [
    "#BEGIN \n",
    "# code here\n",
    "tree = DecisionTree(max_depth=5, min_samples_split=2).fit(features, labels)\n",
    "tests.print_tree(tree, [\"age\", \"salary\", \"resident\", \"siblings\"])\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7b2db68e19a7487511c33d751fd47e2",
     "grade": false,
     "grade_id": "q21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 2: DecisionTree vs DecisionTreeClassifier [6 points]\n",
    "\n",
    "We've just showed that our decision tree is better than the naive NaiveBayes! Let see how it compares to scikit's DecisionTreeClassifier.\n",
    "\n",
    "First, we'll need a fancier dataset. We are going to predict house price levels using the decision trees.\n",
    "\n",
    "We start by loading preprocessed data that we'll use. Since the original House Prices  [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    " is for regression, we have to transform `HousePrices.y_train` and `HousePrices.y_test` to discrete values reflecting price level.\n",
    "\n",
    "|Price range| Label|\n",
    "|:----------:|--:|\n",
    "| $ P < $125000|0|\n",
    "|125000$\\leq P < $ 160000| 1 |\n",
    "|160000$ \\leq P < $ 200000| 2 |\n",
    "|200000$ \\leq P $ | 3 |\n",
    "\n",
    "- 2.1 [3 pts] Start by transforming `y_train` and `y_test` of `house_prices` to discrete values using the provided ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34b73ed8c789e00b4122cfde97fd5fb8",
     "grade": true,
     "grade_id": "a21",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] (1166, 79)\n",
      "[0 1 2 3] (292, 79)\n"
     ]
    }
   ],
   "source": [
    "house_prices = data.HousePrices()\n",
    "#Workspace 2.1\n",
    "#TODO: Discretize y_train and y_test\n",
    "#BEGIN \n",
    "# code here\n",
    "for i in range(len(house_prices.y_train)):\n",
    "    if house_prices.y_train[i] < 125000:\n",
    "        house_prices.y_train[i] = 0\n",
    "    elif house_prices.y_train[i] >= 125000 and house_prices.y_train[i] < 160000:\n",
    "        house_prices.y_train[i] = 1\n",
    "    elif house_prices.y_train[i] >= 160000 and house_prices.y_train[i] < 200000:\n",
    "        house_prices.y_train[i] = 2\n",
    "    elif house_prices.y_train[i] >= 200000:\n",
    "        house_prices.y_train[i] = 3\n",
    "for i in range(len(house_prices.y_test)):\n",
    "    if house_prices.y_test[i] < 125000:\n",
    "        house_prices.y_test[i] = 0\n",
    "    elif house_prices.y_test[i] >= 125000 and house_prices.y_test[i] < 160000:\n",
    "        house_prices.y_test[i] = 1\n",
    "    elif house_prices.y_test[i] >= 160000 and house_prices.y_test[i] < 200000:\n",
    "        house_prices.y_test[i] = 2\n",
    "    elif house_prices.y_test[i] >= 200000:\n",
    "        house_prices.y_test[i] = 3\n",
    "#END\n",
    "print(np.unique(house_prices.y_train), house_prices.X_train.shape)\n",
    "print(np.unique(house_prices.y_test), house_prices.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e57a0d568601251c203ac3cb20ab993",
     "grade": false,
     "grade_id": "q22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 2.2 [3 pts] Compare our `DecisionTree` and scikit's `DecisionTreeClassifier` on the house prices dataset by reporting the accuracies on the test data.\n",
    "\n",
    " [scikit's `DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "uses Gini index by default and only splits based on random subset of features (set `max_features` to None to use all features),\n",
    "so refer to the documentation to change the impurity measure to entropy.\n",
    "\n",
    "Use `max_depth = 5, min_samples_split=2` for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e54fe111567d3f735838e8f83a8b9973",
     "grade": true,
     "grade_id": "a22",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My DecisionTree Accuracy:  0.7123287671232876\n",
      "Sklearn DecisionTreeClassifier Accuracy:  0.7123287671232876\n"
     ]
    }
   ],
   "source": [
    "# Workspace 2.2\n",
    "#BEGIN \n",
    "# code here\n",
    "tree_House = DecisionTree(max_depth=5, min_samples_split=2).fit(house_prices.X_train, house_prices.y_train)\n",
    "accuracy_House = tree_House.score(house_prices.X_test, house_prices.y_test)\n",
    "tree_Sklearn = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=2, max_features = None).fit(house_prices.X_train, house_prices.y_train)\n",
    "accuracy_Sklearn = tree_Sklearn.score(house_prices.X_test, house_prices.y_test)\n",
    "print(\"My DecisionTree Accuracy: \", accuracy_House)\n",
    "print(\"Sklearn DecisionTreeClassifier Accuracy: \", accuracy_Sklearn)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4933229771d9f7671d8df0278af7b4d9",
     "grade": false,
     "grade_id": "q23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Bonus questions\n",
    "We've implemented `DecisionTree` to handle different measures of impurity. We want now to compare our implementation\n",
    "to the standard `DecisionTreeClassifier` using Gini index.\n",
    "- **(Bonus)** 2.3  [2 pts] Complete `gini` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92aa01c6f3d2afe8836b3640df37cac2",
     "grade": true,
     "grade_id": "a23",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    \"\"\"\n",
    "    :param y: 1-d array contains labels, of shape (num_points,)\n",
    "    :return: float, gini index the labels\n",
    "    \"\"\"\n",
    "    gini_index = 0\n",
    "    # Workspace 2.3\n",
    "    #TODO: Compute the gini index of the labels in y\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    label_arr = []\n",
    "    label_prob = []\n",
    "    for i in y:\n",
    "        if i in label_arr:\n",
    "            continue\n",
    "        else:\n",
    "            label_arr.append(i)\n",
    "    for i in label_arr:\n",
    "        num_label = 0\n",
    "        for j in y:\n",
    "            if j == i:\n",
    "                num_label += 1\n",
    "        label_prob.append(num_label / len(y))\n",
    "    square_arr = np.square(label_prob)\n",
    "    gini_index = 1 - (sum(square_arr))\n",
    "    #END\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48611111111111105"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1])\n",
    "gini(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffce63649e045f7c29ed520d1f57aa4f",
     "grade": false,
     "grade_id": "q24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **(Bonus)** 2.4 [2 pts] Perform the same comparison as in 2.2 with gini index but without setting `max_features` to `None`. How do you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d9411484d9bac971d2b86f534f236f9",
     "grade": true,
     "grade_id": "a24a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My DecisionTree Accuracy:  0.6986301369863014\n",
      "Sklearn DecisionTreeClassifier Accuracy:  0.636986301369863\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2) # to fix the randomness in DecisionTreeClassifier\n",
    "# Workspace 2.4.a\n",
    "#BEGIN \n",
    "# code here\n",
    "tree_House2 = DecisionTree(max_depth=5, min_samples_split=2, impurity_measure=gini).fit(house_prices.X_train, house_prices.y_train)\n",
    "accuracy_House2 = tree_House2.score(house_prices.X_test, house_prices.y_test)\n",
    "tree_Sklearn2 = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_split=2, max_features = \"auto\").fit(house_prices.X_train, house_prices.y_train)\n",
    "accuracy_Sklearn2 = tree_Sklearn2.score(house_prices.X_test, house_prices.y_test)\n",
    "print(\"My DecisionTree Accuracy: \", accuracy_House2)\n",
    "print(\"Sklearn DecisionTreeClassifier Accuracy: \", accuracy_Sklearn2)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abd5ea13f0b1340ddf14936399ebedf0",
     "grade": true,
     "grade_id": "a24b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Write-up for 2.4.b\n",
    "%BEGIN\n",
    "\n",
    "Our DecisionTree's accuracy is above Sklearn's because Sklearn's classifier is considering the square root of the number of features when looking for the best split rather than the max number of features which increases the likelyhood that gini inpurity will be higher for each split eventually resulting in an lower overall accuracy.\n",
    "\n",
    "%END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40d5a888077f8ddbea944ef605b69a3f",
     "grade": false,
     "grade_id": "q31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 3 - Model Selection via Cross Validation [16 points]\n",
    "***\n",
    "In this problem, we will be working with scikit-learn `DecisionTreeClassifier`. We want to figure out the best `max_depth` for our dataset.\n",
    "\n",
    "In the house prices dataset, we only have a training set and a test set. The question then is how do we perform the model selection seen in Problem Set 1?\n",
    "\n",
    "One way to do so is via **the cross validation set approach** which basically means setting aside a portion of our training data to use as a validation set.\n",
    "The goal is to use the validation set to find the best hyperparameters for our model (`max_depth` in the case of decision trees).\n",
    "\n",
    "- 3.1 [3 points] complete the `cross_validate` function to train the classifier on the training set and return the accuracy on the validation set based on provided indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb9b6096d9f2ecbcd8d9c331ea8467ee",
     "grade": true,
     "grade_id": "a31",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validate(classifier, X, y, train_indices, valid_indices):\n",
    "    \"\"\"\n",
    "    Train classifier on training set and validate on the validation set\n",
    "    :param classifier: the classifier to use\n",
    "    :param X: all data of shape (num_samples, num_features)\n",
    "    :param y: all labels of shape (num_samples)\n",
    "    :param train_indices:  indices to be used for training the model\n",
    "    :param valid_indices:  indices to be used for validating the model\n",
    "    :return: he accuracy of the classifier on the validation set\n",
    "    \"\"\"\n",
    "    valid_accuracy = 0\n",
    "    #Workspace 3.1\n",
    "    #TODO: train and validate the model based on provided indices\n",
    "    #Hint: use score method of the classifier\n",
    "    #BEGIN \n",
    "    classifier.fit(X[train_indices],y[train_indices])\n",
    "    valid_accuracy = classifier.score(X[valid_indices],y[valid_indices])\n",
    "    # code here\n",
    "    #END\n",
    "    return valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cea5adf3217a3118305ca599da8ad723",
     "grade": false,
     "grade_id": "q32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 3.2 [2 points] Report the validation accuracy using the validation set approach for scikit-learn `DecisionTreeClassifier` with `max_depth=3`\n",
    " when using the last 100 training points as a validation set and the rest as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736c9be3f2b43025375649f8426c5eb6",
     "grade": true,
     "grade_id": "a32",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for scikit-learn DecisionTreeClassifier: 0.06\n"
     ]
    }
   ],
   "source": [
    "#Workspace 3.2\n",
    "#TODO: Report the cross validation accuracy using the last 100 training points as validation set\n",
    "#and the rest of the training points as training\n",
    "#BEGIN \n",
    "# code here\n",
    "house_prices = data.HousePrices()\n",
    "train_indices = []\n",
    "valid_indices = []\n",
    "for i in range(len(house_prices.y_train) - 100):\n",
    "    train_indices.append(i)\n",
    "last_100 = len(house_prices.y_train) - (len(house_prices.y_train) - 100)\n",
    "for i in range(last_100):\n",
    "    valid_indices.append(i)\n",
    "classifier = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_split=2, max_features = None)\n",
    "report = cross_validate(classifier, house_prices.X_train, house_prices.y_train, train_indices, valid_indices)\n",
    "print(\"Validation accuracy for scikit-learn DecisionTreeClassifier:\", report)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83ce3fa0c7dae7a8cebb1d3b912ea860",
     "grade": false,
     "grade_id": "q33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The issue with the validation set approach is that we're reducing the size of our training data,\n",
    " and the lower number of samples implies higher uncertainty.\n",
    "\n",
    "A work-around is to use *k-fold cross validation*.\n",
    "We start by partitioning the training data into k different and equally size partitions.\n",
    "Then for each of the k runs, we keep a different chunk for the validation while using the remaining k-1 for training.\n",
    "We note the validation accuracy during each of the k runs.\n",
    "\n",
    "After each of the k-folds has been used as a validation set, the average of the k recorded accuracies becomes the performance of our model.\n",
    "The k-fold cross validation method gives us a better estimate on how well the model would perform on new unseen data\n",
    " (test set) while allowing it to train on a larger portion of the dataset.\n",
    "- 3.3 [5 points] Complete `k_fold_cv`. Use the helper function `partition_to_k` that generates the partition of indices to k different chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25c6c76e733b03a0955c74129bd62156",
     "grade": true,
     "grade_id": "a33",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def partition_to_k(permutation, k):\n",
    "    \"\"\"\n",
    "    Partition permutation (shuffled indices) to k different chunks and generate the train/valid splits for the k-fold\n",
    "    :param permutation: shuffles indices\n",
    "    :param k: number of folds\n",
    "    :return: iterable of different k partitions\n",
    "    \"\"\"\n",
    "    size = int(np.ceil(len(permutation)/k))\n",
    "    for i in range(0, len(permutation), size):\n",
    "        # valid indices j for which i < o(i) < i + size\n",
    "        valid_indices = np.where(np.logical_and(permutation>i, permutation<= i + size))[0]\n",
    "        # train indices j for which o(j) <= j  or o(j) >= j + size\n",
    "        train_indices = np.where(~np.logical_and(permutation>i, permutation<= i + size))[0]\n",
    "        yield train_indices, valid_indices\n",
    "\n",
    "def k_fold_cv(classifier, k, X, y):\n",
    "    \"\"\"\n",
    "    This function performs k-fold cross validation\n",
    "    :param classifier: a classifier to be used\n",
    "    :param k: number of folds\n",
    "    :param X: all training data of shape (num_samples, num_features)\n",
    "    :param y: all labels of shape (num_samples)\n",
    "    :return: the average accuracy of the classifier in k-runs\n",
    "    \"\"\"\n",
    "    #shuffle data indices\n",
    "    permutation = np.random.RandomState(seed=42).permutation(range(X.shape[0]))\n",
    "    mean_accuracy = 0\n",
    "    #Workspace 3.3\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    k_Part = partition_to_k(permutation, k)\n",
    "    classifier.fit(X,y)\n",
    "    for x in k_Part:\n",
    "        mean_accuracy += classifier.score\n",
    "        print(x)\n",
    "    #END\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   0,    2,    3,    4,    5,    7,    9,   10,   11,   12,   13,\n",
      "         14,   17,   20,   22,   23,   24,   26,   28,   29,   31,   34,\n",
      "         35,   38,   39,   40,   41,   42,   43,   44,   45,   46,   47,\n",
      "         48,   49,   50,   51,   52,   53,   54,   55,   56,   58,   60,\n",
      "         61,   62,   64,   65,   66,   67,   68,   69,   70,   72,   73,\n",
      "         76,   77,   78,   79,   81,   82,   83,   84,   85,   86,   87,\n",
      "         89,   91,   92,   93,   95,   97,   98,   99,  100,  101,  102,\n",
      "        104,  105,  106,  107,  108,  109,  110,  112,  113,  114,  115,\n",
      "        116,  117,  118,  119,  120,  121,  122,  123,  125,  126,  127,\n",
      "        129,  131,  132,  133,  134,  135,  136,  137,  139,  140,  141,\n",
      "        142,  143,  144,  145,  146,  147,  148,  149,  151,  152,  155,\n",
      "        156,  157,  158,  159,  160,  161,  162,  163,  164,  165,  166,\n",
      "        167,  168,  169,  170,  172,  173,  174,  175,  176,  177,  178,\n",
      "        179,  181,  182,  183,  184,  185,  186,  187,  188,  189,  190,\n",
      "        191,  192,  193,  195,  197,  198,  199,  202,  203,  204,  206,\n",
      "        207,  208,  209,  210,  211,  212,  213,  215,  216,  217,  219,\n",
      "        220,  221,  223,  225,  226,  227,  228,  230,  231,  232,  233,\n",
      "        234,  235,  236,  237,  238,  239,  242,  245,  246,  247,  248,\n",
      "        249,  250,  251,  253,  254,  255,  256,  257,  258,  259,  260,\n",
      "        261,  263,  264,  265,  266,  267,  268,  269,  270,  271,  272,\n",
      "        273,  274,  277,  278,  280,  281,  282,  283,  284,  285,  286,\n",
      "        287,  288,  289,  290,  291,  292,  293,  294,  295,  296,  299,\n",
      "        300,  301,  303,  304,  305,  306,  307,  308,  309,  310,  311,\n",
      "        312,  313,  314,  315,  316,  318,  320,  323,  326,  327,  328,\n",
      "        330,  331,  332,  333,  334,  335,  336,  337,  338,  339,  340,\n",
      "        341,  342,  343,  344,  345,  346,  347,  349,  351,  352,  353,\n",
      "        354,  355,  356,  357,  358,  359,  361,  362,  364,  365,  366,\n",
      "        367,  368,  369,  370,  371,  372,  373,  374,  375,  376,  378,\n",
      "        379,  380,  381,  382,  383,  385,  387,  388,  389,  390,  391,\n",
      "        392,  393,  394,  395,  396,  397,  398,  399,  400,  402,  403,\n",
      "        404,  405,  406,  407,  408,  409,  410,  411,  413,  414,  415,\n",
      "        416,  417,  418,  419,  420,  421,  422,  423,  424,  427,  429,\n",
      "        430,  431,  433,  434,  435,  436,  439,  441,  442,  443,  444,\n",
      "        445,  446,  447,  448,  449,  450,  452,  453,  454,  455,  458,\n",
      "        459,  460,  461,  462,  463,  465,  466,  467,  468,  469,  472,\n",
      "        473,  474,  476,  477,  478,  479,  480,  481,  483,  484,  485,\n",
      "        487,  488,  490,  491,  492,  493,  494,  495,  496,  497,  498,\n",
      "        500,  501,  503,  504,  505,  506,  508,  509,  510,  513,  514,\n",
      "        515,  516,  518,  519,  520,  521,  522,  523,  524,  525,  526,\n",
      "        527,  528,  529,  530,  531,  532,  533,  534,  535,  536,  538,\n",
      "        542,  543,  545,  546,  547,  548,  549,  550,  551,  552,  553,\n",
      "        554,  555,  556,  557,  558,  559,  561,  563,  564,  566,  568,\n",
      "        569,  570,  573,  574,  575,  576,  577,  578,  579,  580,  581,\n",
      "        582,  585,  587,  589,  591,  592,  593,  594,  596,  597,  600,\n",
      "        602,  603,  607,  608,  609,  610,  611,  612,  613,  615,  616,\n",
      "        617,  618,  619,  620,  621,  622,  623,  624,  625,  626,  627,\n",
      "        628,  629,  632,  633,  634,  636,  637,  638,  639,  640,  641,\n",
      "        642,  643,  644,  645,  646,  647,  648,  649,  650,  651,  652,\n",
      "        654,  655,  656,  657,  658,  660,  661,  662,  664,  665,  666,\n",
      "        667,  668,  670,  671,  672,  675,  676,  677,  678,  679,  680,\n",
      "        681,  682,  683,  684,  685,  686,  687,  688,  690,  691,  692,\n",
      "        693,  694,  698,  700,  701,  702,  703,  704,  705,  706,  707,\n",
      "        710,  712,  714,  715,  716,  718,  719,  722,  723,  724,  725,\n",
      "        727,  728,  729,  730,  731,  733,  734,  735,  736,  737,  738,\n",
      "        739,  742,  743,  744,  746,  747,  748,  749,  750,  752,  753,\n",
      "        754,  755,  757,  761,  762,  763,  764,  765,  766,  767,  768,\n",
      "        769,  770,  771,  772,  773,  776,  777,  778,  779,  781,  782,\n",
      "        783,  785,  786,  787,  788,  789,  790,  792,  794,  795,  796,\n",
      "        797,  798,  799,  800,  803,  804,  805,  806,  807,  808,  810,\n",
      "        811,  812,  813,  814,  815,  816,  817,  818,  819,  820,  823,\n",
      "        824,  825,  826,  829,  830,  831,  833,  834,  835,  836,  837,\n",
      "        838,  839,  840,  841,  843,  845,  848,  851,  852,  853,  854,\n",
      "        855,  856,  857,  858,  859,  860,  861,  864,  865,  866,  867,\n",
      "        868,  871,  872,  873,  874,  875,  876,  877,  879,  880,  882,\n",
      "        883,  884,  885,  886,  887,  888,  889,  891,  892,  894,  895,\n",
      "        896,  897,  898,  899,  901,  902,  903,  906,  907,  909,  910,\n",
      "        911,  912,  913,  914,  918,  919,  921,  922,  923,  928,  929,\n",
      "        930,  931,  932,  933,  934,  935,  936,  937,  938,  939,  940,\n",
      "        941,  942,  943,  945,  946,  947,  948,  949,  950,  951,  952,\n",
      "        953,  954,  955,  956,  957,  958,  961,  962,  963,  964,  965,\n",
      "        966,  967,  968,  969,  970,  971,  974,  975,  977,  978,  979,\n",
      "        981,  982,  983,  984,  985,  986,  987,  991,  992,  993,  994,\n",
      "        995,  996,  997,  998, 1001, 1002, 1003, 1005, 1006, 1007, 1008,\n",
      "       1010, 1012, 1013, 1015, 1016, 1017, 1019, 1020, 1021, 1022, 1023,\n",
      "       1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034,\n",
      "       1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045,\n",
      "       1047, 1048, 1049, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058,\n",
      "       1059, 1060, 1061, 1062, 1063, 1064, 1065, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1075, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085,\n",
      "       1088, 1089, 1090, 1092, 1094, 1095, 1096, 1097, 1098, 1099, 1100,\n",
      "       1101, 1102, 1103, 1104, 1107, 1108, 1109, 1110, 1111, 1112, 1113,\n",
      "       1116, 1117, 1118, 1120, 1121, 1122, 1123, 1125, 1126, 1127, 1129,\n",
      "       1131, 1132, 1133, 1134, 1135, 1136, 1138, 1139, 1140, 1141, 1142,\n",
      "       1143, 1144, 1145, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1155,\n",
      "       1156, 1158, 1159, 1161, 1162, 1163, 1164, 1165], dtype=int64), array([   1,    6,    8,   15,   16,   18,   19,   21,   25,   27,   30,\n",
      "         32,   33,   36,   37,   57,   59,   63,   71,   74,   75,   80,\n",
      "         88,   90,   94,   96,  103,  111,  124,  128,  130,  138,  150,\n",
      "        153,  154,  171,  180,  194,  196,  200,  201,  205,  214,  218,\n",
      "        222,  224,  229,  240,  241,  243,  244,  252,  262,  275,  276,\n",
      "        279,  297,  298,  302,  317,  319,  321,  322,  324,  325,  329,\n",
      "        348,  350,  360,  363,  377,  384,  386,  401,  412,  425,  426,\n",
      "        428,  432,  437,  438,  440,  451,  456,  457,  464,  470,  471,\n",
      "        475,  482,  486,  489,  499,  502,  507,  511,  512,  517,  537,\n",
      "        539,  540,  541,  544,  560,  562,  565,  567,  571,  572,  583,\n",
      "        584,  586,  588,  590,  595,  598,  599,  601,  604,  605,  606,\n",
      "        614,  630,  631,  635,  653,  659,  663,  669,  673,  674,  689,\n",
      "        695,  696,  697,  699,  708,  709,  711,  713,  717,  720,  721,\n",
      "        726,  732,  740,  741,  745,  751,  756,  758,  759,  760,  774,\n",
      "        775,  780,  784,  791,  793,  801,  802,  809,  821,  822,  827,\n",
      "        828,  832,  842,  844,  846,  847,  849,  850,  862,  863,  869,\n",
      "        870,  878,  881,  890,  893,  900,  904,  905,  908,  915,  916,\n",
      "        917,  920,  924,  925,  926,  927,  944,  959,  960,  972,  973,\n",
      "        976,  980,  988,  989,  990,  999, 1000, 1004, 1009, 1011, 1014,\n",
      "       1018, 1046, 1053, 1066, 1073, 1074, 1076, 1078, 1086, 1087, 1091,\n",
      "       1093, 1105, 1106, 1114, 1115, 1119, 1124, 1128, 1130, 1137, 1146,\n",
      "       1154, 1157, 1160], dtype=int64))\n",
      "(array([   0,    1,    2,    3,    4,    6,    7,    8,   10,   12,   13,\n",
      "         14,   15,   16,   17,   18,   19,   20,   21,   22,   23,   25,\n",
      "         26,   27,   28,   29,   30,   31,   32,   33,   34,   35,   36,\n",
      "         37,   38,   39,   40,   44,   45,   46,   47,   48,   49,   51,\n",
      "         52,   53,   55,   57,   58,   59,   60,   61,   62,   63,   64,\n",
      "         67,   68,   69,   70,   71,   72,   73,   74,   75,   78,   80,\n",
      "         82,   83,   84,   85,   88,   89,   90,   91,   93,   94,   95,\n",
      "         96,   98,  100,  101,  102,  103,  105,  106,  110,  111,  113,\n",
      "        114,  116,  118,  121,  122,  124,  126,  127,  128,  129,  130,\n",
      "        131,  132,  135,  136,  138,  139,  140,  142,  143,  144,  145,\n",
      "        146,  147,  148,  149,  150,  152,  153,  154,  155,  156,  158,\n",
      "        159,  160,  162,  163,  164,  165,  166,  167,  168,  169,  170,\n",
      "        171,  172,  173,  174,  175,  176,  177,  179,  180,  181,  182,\n",
      "        183,  184,  185,  189,  190,  191,  192,  193,  194,  195,  196,\n",
      "        197,  198,  199,  200,  201,  202,  203,  204,  205,  207,  208,\n",
      "        209,  211,  212,  213,  214,  216,  217,  218,  219,  220,  221,\n",
      "        222,  224,  226,  228,  229,  230,  232,  233,  234,  235,  236,\n",
      "        237,  238,  239,  240,  241,  242,  243,  244,  245,  247,  248,\n",
      "        249,  251,  252,  253,  254,  255,  256,  257,  259,  260,  262,\n",
      "        263,  264,  265,  266,  267,  269,  270,  272,  273,  275,  276,\n",
      "        278,  279,  281,  283,  284,  285,  287,  288,  289,  291,  292,\n",
      "        297,  298,  299,  302,  304,  305,  306,  307,  308,  309,  311,\n",
      "        312,  313,  317,  318,  319,  321,  322,  323,  324,  325,  326,\n",
      "        329,  330,  331,  332,  334,  335,  336,  337,  339,  340,  342,\n",
      "        343,  344,  345,  347,  348,  349,  350,  351,  354,  357,  358,\n",
      "        359,  360,  361,  362,  363,  364,  365,  366,  367,  369,  372,\n",
      "        374,  375,  376,  377,  378,  380,  381,  382,  383,  384,  386,\n",
      "        391,  392,  394,  395,  396,  397,  398,  399,  400,  401,  402,\n",
      "        403,  404,  405,  406,  408,  409,  411,  412,  415,  416,  418,\n",
      "        419,  420,  421,  423,  424,  425,  426,  427,  428,  429,  430,\n",
      "        432,  433,  434,  435,  437,  438,  439,  440,  441,  442,  443,\n",
      "        444,  446,  447,  448,  450,  451,  454,  455,  456,  457,  458,\n",
      "        459,  460,  462,  463,  464,  466,  468,  470,  471,  472,  474,\n",
      "        475,  476,  477,  479,  482,  484,  486,  487,  489,  491,  492,\n",
      "        493,  494,  495,  497,  499,  500,  501,  502,  503,  504,  506,\n",
      "        507,  508,  509,  510,  511,  512,  513,  514,  515,  516,  517,\n",
      "        518,  519,  522,  523,  525,  526,  527,  528,  529,  530,  531,\n",
      "        532,  533,  535,  536,  537,  538,  539,  540,  541,  542,  543,\n",
      "        544,  545,  547,  548,  549,  550,  551,  553,  554,  556,  557,\n",
      "        558,  559,  560,  562,  563,  564,  565,  566,  567,  568,  569,\n",
      "        571,  572,  574,  576,  577,  578,  579,  580,  582,  583,  584,\n",
      "        585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,\n",
      "        596,  597,  598,  599,  600,  601,  602,  603,  604,  605,  606,\n",
      "        607,  608,  609,  610,  611,  612,  614,  615,  616,  617,  618,\n",
      "        620,  621,  625,  626,  629,  630,  631,  632,  633,  634,  635,\n",
      "        637,  639,  641,  642,  643,  644,  645,  646,  649,  650,  653,\n",
      "        654,  655,  656,  657,  658,  659,  661,  662,  663,  664,  665,\n",
      "        666,  667,  668,  669,  671,  672,  673,  674,  676,  677,  678,\n",
      "        680,  681,  682,  683,  684,  685,  686,  689,  690,  692,  693,\n",
      "        694,  695,  696,  697,  698,  699,  701,  702,  703,  704,  707,\n",
      "        708,  709,  710,  711,  712,  713,  714,  716,  717,  719,  720,\n",
      "        721,  722,  723,  725,  726,  729,  730,  731,  732,  733,  734,\n",
      "        735,  736,  737,  738,  739,  740,  741,  742,  743,  744,  745,\n",
      "        746,  747,  748,  749,  750,  751,  752,  755,  756,  758,  759,\n",
      "        760,  761,  762,  763,  765,  767,  768,  769,  772,  774,  775,\n",
      "        776,  777,  778,  779,  780,  781,  783,  784,  785,  786,  788,\n",
      "        789,  791,  792,  793,  794,  795,  797,  798,  799,  800,  801,\n",
      "        802,  804,  805,  806,  807,  808,  809,  810,  811,  813,  814,\n",
      "        815,  816,  817,  818,  819,  820,  821,  822,  825,  826,  827,\n",
      "        828,  829,  830,  831,  832,  834,  835,  836,  837,  838,  839,\n",
      "        840,  842,  843,  844,  845,  846,  847,  848,  849,  850,  851,\n",
      "        852,  853,  854,  855,  856,  857,  859,  860,  861,  862,  863,\n",
      "        865,  867,  869,  870,  871,  872,  873,  875,  876,  877,  878,\n",
      "        879,  880,  881,  882,  883,  884,  885,  886,  887,  888,  889,\n",
      "        890,  891,  892,  893,  894,  895,  896,  897,  898,  899,  900,\n",
      "        901,  902,  903,  904,  905,  906,  907,  908,  911,  912,  913,\n",
      "        914,  915,  916,  917,  919,  920,  921,  922,  923,  924,  925,\n",
      "        926,  927,  929,  932,  933,  934,  935,  936,  937,  938,  939,\n",
      "        941,  944,  945,  946,  947,  948,  949,  950,  951,  952,  953,\n",
      "        954,  955,  956,  957,  958,  959,  960,  961,  963,  964,  965,\n",
      "        966,  967,  968,  972,  973,  974,  975,  976,  977,  978,  979,\n",
      "        980,  981,  982,  983,  986,  988,  989,  990,  992,  995,  996,\n",
      "        997,  998,  999, 1000, 1001, 1004, 1005, 1006, 1007, 1008, 1009,\n",
      "       1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1020, 1021,\n",
      "       1022, 1024, 1026, 1027, 1029, 1030, 1031, 1032, 1033, 1034, 1035,\n",
      "       1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046,\n",
      "       1047, 1048, 1049, 1050, 1051, 1052, 1053, 1055, 1056, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1069, 1070, 1071,\n",
      "       1072, 1073, 1074, 1075, 1076, 1077, 1078, 1083, 1084, 1085, 1086,\n",
      "       1087, 1088, 1089, 1091, 1092, 1093, 1094, 1095, 1096, 1098, 1100,\n",
      "       1102, 1104, 1105, 1106, 1107, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1124, 1126, 1128,\n",
      "       1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139,\n",
      "       1140, 1141, 1142, 1143, 1144, 1146, 1149, 1151, 1153, 1154, 1155,\n",
      "       1156, 1157, 1160, 1161, 1162, 1163, 1164, 1165], dtype=int64), array([   5,    9,   11,   24,   41,   42,   43,   50,   54,   56,   65,\n",
      "         66,   76,   77,   79,   81,   86,   87,   92,   97,   99,  104,\n",
      "        107,  108,  109,  112,  115,  117,  119,  120,  123,  125,  133,\n",
      "        134,  137,  141,  151,  157,  161,  178,  186,  187,  188,  206,\n",
      "        210,  215,  223,  225,  227,  231,  246,  250,  258,  261,  268,\n",
      "        271,  274,  277,  280,  282,  286,  290,  293,  294,  295,  296,\n",
      "        300,  301,  303,  310,  314,  315,  316,  320,  327,  328,  333,\n",
      "        338,  341,  346,  352,  353,  355,  356,  368,  370,  371,  373,\n",
      "        379,  385,  387,  388,  389,  390,  393,  407,  410,  413,  414,\n",
      "        417,  422,  431,  436,  445,  449,  452,  453,  461,  465,  467,\n",
      "        469,  473,  478,  480,  481,  483,  485,  488,  490,  496,  498,\n",
      "        505,  520,  521,  524,  534,  546,  552,  555,  561,  570,  573,\n",
      "        575,  581,  613,  619,  622,  623,  624,  627,  628,  636,  638,\n",
      "        640,  647,  648,  651,  652,  660,  670,  675,  679,  687,  688,\n",
      "        691,  700,  705,  706,  715,  718,  724,  727,  728,  753,  754,\n",
      "        757,  764,  766,  770,  771,  773,  782,  787,  790,  796,  803,\n",
      "        812,  823,  824,  833,  841,  858,  864,  866,  868,  874,  909,\n",
      "        910,  918,  928,  930,  931,  940,  942,  943,  962,  969,  970,\n",
      "        971,  984,  985,  987,  991,  993,  994, 1002, 1003, 1019, 1023,\n",
      "       1025, 1028, 1054, 1057, 1068, 1079, 1080, 1081, 1082, 1090, 1097,\n",
      "       1099, 1101, 1103, 1108, 1123, 1125, 1127, 1145, 1147, 1148, 1150,\n",
      "       1152, 1158, 1159], dtype=int64))\n",
      "(array([   0,    1,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
      "         13,   15,   16,   17,   18,   19,   21,   22,   23,   24,   25,\n",
      "         26,   27,   28,   29,   30,   31,   32,   33,   34,   35,   36,\n",
      "         37,   38,   39,   40,   41,   42,   43,   44,   45,   46,   47,\n",
      "         48,   49,   50,   52,   54,   55,   56,   57,   58,   59,   60,\n",
      "         61,   62,   63,   64,   65,   66,   68,   70,   71,   72,   73,\n",
      "         74,   75,   76,   77,   78,   79,   80,   81,   83,   86,   87,\n",
      "         88,   89,   90,   92,   93,   94,   95,   96,   97,   98,   99,\n",
      "        100,  103,  104,  105,  107,  108,  109,  111,  112,  114,  115,\n",
      "        116,  117,  118,  119,  120,  122,  123,  124,  125,  126,  127,\n",
      "        128,  129,  130,  132,  133,  134,  135,  137,  138,  141,  142,\n",
      "        143,  144,  145,  146,  147,  148,  149,  150,  151,  153,  154,\n",
      "        155,  156,  157,  158,  160,  161,  162,  164,  167,  169,  170,\n",
      "        171,  172,  175,  176,  177,  178,  179,  180,  181,  183,  184,\n",
      "        185,  186,  187,  188,  192,  193,  194,  195,  196,  198,  199,\n",
      "        200,  201,  205,  206,  207,  209,  210,  212,  214,  215,  216,\n",
      "        217,  218,  220,  221,  222,  223,  224,  225,  226,  227,  228,\n",
      "        229,  231,  233,  235,  237,  238,  240,  241,  242,  243,  244,\n",
      "        245,  246,  247,  248,  250,  251,  252,  253,  255,  256,  257,\n",
      "        258,  261,  262,  266,  267,  268,  269,  271,  274,  275,  276,\n",
      "        277,  278,  279,  280,  282,  283,  284,  285,  286,  290,  291,\n",
      "        293,  294,  295,  296,  297,  298,  299,  300,  301,  302,  303,\n",
      "        304,  305,  306,  307,  308,  309,  310,  311,  313,  314,  315,\n",
      "        316,  317,  318,  319,  320,  321,  322,  323,  324,  325,  327,\n",
      "        328,  329,  331,  332,  333,  337,  338,  339,  340,  341,  343,\n",
      "        344,  346,  348,  349,  350,  351,  352,  353,  354,  355,  356,\n",
      "        357,  360,  361,  362,  363,  364,  367,  368,  369,  370,  371,\n",
      "        373,  375,  376,  377,  378,  379,  380,  381,  382,  383,  384,\n",
      "        385,  386,  387,  388,  389,  390,  391,  392,  393,  395,  396,\n",
      "        399,  400,  401,  402,  403,  404,  405,  407,  408,  409,  410,\n",
      "        412,  413,  414,  415,  417,  418,  419,  420,  421,  422,  423,\n",
      "        425,  426,  428,  429,  431,  432,  434,  436,  437,  438,  439,\n",
      "        440,  441,  443,  444,  445,  446,  447,  449,  451,  452,  453,\n",
      "        454,  456,  457,  458,  460,  461,  463,  464,  465,  466,  467,\n",
      "        469,  470,  471,  472,  473,  474,  475,  476,  477,  478,  479,\n",
      "        480,  481,  482,  483,  484,  485,  486,  487,  488,  489,  490,\n",
      "        492,  493,  494,  496,  498,  499,  501,  502,  503,  505,  506,\n",
      "        507,  508,  510,  511,  512,  513,  514,  516,  517,  519,  520,\n",
      "        521,  522,  523,  524,  525,  526,  527,  528,  530,  532,  533,\n",
      "        534,  536,  537,  538,  539,  540,  541,  544,  545,  546,  547,\n",
      "        549,  551,  552,  553,  554,  555,  556,  558,  559,  560,  561,\n",
      "        562,  565,  566,  567,  568,  569,  570,  571,  572,  573,  574,\n",
      "        575,  576,  578,  579,  581,  582,  583,  584,  586,  587,  588,\n",
      "        590,  592,  595,  596,  598,  599,  600,  601,  603,  604,  605,\n",
      "        606,  607,  610,  611,  613,  614,  616,  617,  618,  619,  620,\n",
      "        621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
      "        633,  635,  636,  637,  638,  639,  640,  641,  642,  643,  645,\n",
      "        646,  647,  648,  649,  650,  651,  652,  653,  654,  655,  657,\n",
      "        659,  660,  663,  664,  666,  667,  668,  669,  670,  671,  673,\n",
      "        674,  675,  677,  678,  679,  681,  683,  684,  685,  686,  687,\n",
      "        688,  689,  690,  691,  693,  694,  695,  696,  697,  699,  700,\n",
      "        701,  702,  704,  705,  706,  707,  708,  709,  710,  711,  712,\n",
      "        713,  714,  715,  717,  718,  719,  720,  721,  722,  724,  725,\n",
      "        726,  727,  728,  730,  732,  733,  734,  737,  738,  739,  740,\n",
      "        741,  743,  744,  745,  747,  749,  750,  751,  753,  754,  756,\n",
      "        757,  758,  759,  760,  762,  763,  764,  765,  766,  767,  768,\n",
      "        770,  771,  772,  773,  774,  775,  776,  780,  781,  782,  783,\n",
      "        784,  787,  788,  789,  790,  791,  793,  795,  796,  798,  799,\n",
      "        800,  801,  802,  803,  804,  805,  806,  807,  809,  812,  814,\n",
      "        816,  818,  819,  820,  821,  822,  823,  824,  825,  826,  827,\n",
      "        828,  829,  830,  832,  833,  835,  836,  837,  838,  839,  841,\n",
      "        842,  844,  845,  846,  847,  849,  850,  853,  854,  855,  856,\n",
      "        858,  859,  860,  861,  862,  863,  864,  865,  866,  867,  868,\n",
      "        869,  870,  872,  873,  874,  876,  878,  879,  880,  881,  882,\n",
      "        883,  885,  886,  887,  888,  890,  892,  893,  894,  895,  896,\n",
      "        897,  898,  899,  900,  901,  902,  903,  904,  905,  906,  907,\n",
      "        908,  909,  910,  911,  913,  915,  916,  917,  918,  919,  920,\n",
      "        921,  923,  924,  925,  926,  927,  928,  930,  931,  932,  933,\n",
      "        934,  937,  938,  939,  940,  942,  943,  944,  945,  947,  949,\n",
      "        950,  953,  954,  956,  957,  958,  959,  960,  961,  962,  963,\n",
      "        964,  965,  966,  968,  969,  970,  971,  972,  973,  974,  976,\n",
      "        977,  978,  979,  980,  982,  983,  984,  985,  986,  987,  988,\n",
      "        989,  990,  991,  992,  993,  994,  995,  997,  998,  999, 1000,\n",
      "       1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1011, 1012, 1014,\n",
      "       1015, 1017, 1018, 1019, 1020, 1021, 1023, 1024, 1025, 1027, 1028,\n",
      "       1030, 1031, 1034, 1035, 1037, 1043, 1044, 1046, 1047, 1049, 1050,\n",
      "       1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1063,\n",
      "       1064, 1066, 1067, 1068, 1070, 1071, 1072, 1073, 1074, 1075, 1076,\n",
      "       1077, 1078, 1079, 1080, 1081, 1082, 1086, 1087, 1088, 1090, 1091,\n",
      "       1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102,\n",
      "       1103, 1104, 1105, 1106, 1107, 1108, 1111, 1112, 1113, 1114, 1115,\n",
      "       1116, 1118, 1119, 1120, 1122, 1123, 1124, 1125, 1127, 1128, 1129,\n",
      "       1130, 1132, 1134, 1136, 1137, 1138, 1141, 1143, 1144, 1145, 1146,\n",
      "       1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157,\n",
      "       1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165], dtype=int64), array([   2,   12,   14,   20,   51,   53,   67,   69,   82,   84,   85,\n",
      "         91,  101,  102,  106,  110,  113,  121,  131,  136,  139,  140,\n",
      "        152,  159,  163,  165,  166,  168,  173,  174,  182,  189,  190,\n",
      "        191,  197,  202,  203,  204,  208,  211,  213,  219,  230,  232,\n",
      "        234,  236,  239,  249,  254,  259,  260,  263,  264,  265,  270,\n",
      "        272,  273,  281,  287,  288,  289,  292,  312,  326,  330,  334,\n",
      "        335,  336,  342,  345,  347,  358,  359,  365,  366,  372,  374,\n",
      "        394,  397,  398,  406,  411,  416,  424,  427,  430,  433,  435,\n",
      "        442,  448,  450,  455,  459,  462,  468,  491,  495,  497,  500,\n",
      "        504,  509,  515,  518,  529,  531,  535,  542,  543,  548,  550,\n",
      "        557,  563,  564,  577,  580,  585,  589,  591,  593,  594,  597,\n",
      "        602,  608,  609,  612,  615,  632,  634,  644,  656,  658,  661,\n",
      "        662,  665,  672,  676,  680,  682,  692,  698,  703,  716,  723,\n",
      "        729,  731,  735,  736,  742,  746,  748,  752,  755,  761,  769,\n",
      "        777,  778,  779,  785,  786,  792,  794,  797,  808,  810,  811,\n",
      "        813,  815,  817,  831,  834,  840,  843,  848,  851,  852,  857,\n",
      "        871,  875,  877,  884,  889,  891,  912,  914,  922,  929,  935,\n",
      "        936,  941,  946,  948,  951,  952,  955,  967,  975,  981,  996,\n",
      "       1001, 1010, 1013, 1016, 1022, 1026, 1029, 1032, 1033, 1036, 1038,\n",
      "       1039, 1040, 1041, 1042, 1045, 1048, 1061, 1062, 1065, 1069, 1083,\n",
      "       1084, 1085, 1089, 1109, 1110, 1117, 1121, 1126, 1131, 1133, 1135,\n",
      "       1139, 1140, 1142], dtype=int64))\n",
      "(array([   0,    1,    2,    5,    6,    8,    9,   11,   12,   13,   14,\n",
      "         15,   16,   17,   18,   19,   20,   21,   22,   23,   24,   25,\n",
      "         26,   27,   30,   31,   32,   33,   34,   36,   37,   38,   39,\n",
      "         41,   42,   43,   44,   45,   47,   49,   50,   51,   53,   54,\n",
      "         56,   57,   58,   59,   60,   61,   63,   65,   66,   67,   68,\n",
      "         69,   71,   74,   75,   76,   77,   79,   80,   81,   82,   84,\n",
      "         85,   86,   87,   88,   90,   91,   92,   93,   94,   96,   97,\n",
      "         98,   99,  100,  101,  102,  103,  104,  106,  107,  108,  109,\n",
      "        110,  111,  112,  113,  115,  117,  119,  120,  121,  123,  124,\n",
      "        125,  126,  128,  130,  131,  132,  133,  134,  136,  137,  138,\n",
      "        139,  140,  141,  142,  148,  149,  150,  151,  152,  153,  154,\n",
      "        155,  157,  158,  159,  161,  162,  163,  164,  165,  166,  168,\n",
      "        171,  172,  173,  174,  176,  177,  178,  180,  182,  183,  184,\n",
      "        186,  187,  188,  189,  190,  191,  193,  194,  195,  196,  197,\n",
      "        198,  199,  200,  201,  202,  203,  204,  205,  206,  208,  210,\n",
      "        211,  212,  213,  214,  215,  216,  217,  218,  219,  220,  222,\n",
      "        223,  224,  225,  227,  228,  229,  230,  231,  232,  234,  235,\n",
      "        236,  239,  240,  241,  243,  244,  246,  247,  249,  250,  252,\n",
      "        253,  254,  256,  258,  259,  260,  261,  262,  263,  264,  265,\n",
      "        267,  268,  270,  271,  272,  273,  274,  275,  276,  277,  278,\n",
      "        279,  280,  281,  282,  284,  286,  287,  288,  289,  290,  291,\n",
      "        292,  293,  294,  295,  296,  297,  298,  299,  300,  301,  302,\n",
      "        303,  305,  306,  309,  310,  312,  313,  314,  315,  316,  317,\n",
      "        318,  319,  320,  321,  322,  324,  325,  326,  327,  328,  329,\n",
      "        330,  333,  334,  335,  336,  338,  339,  341,  342,  345,  346,\n",
      "        347,  348,  349,  350,  352,  353,  355,  356,  357,  358,  359,\n",
      "        360,  361,  362,  363,  364,  365,  366,  368,  369,  370,  371,\n",
      "        372,  373,  374,  375,  376,  377,  378,  379,  380,  382,  384,\n",
      "        385,  386,  387,  388,  389,  390,  393,  394,  395,  397,  398,\n",
      "        401,  403,  405,  406,  407,  410,  411,  412,  413,  414,  416,\n",
      "        417,  418,  421,  422,  424,  425,  426,  427,  428,  430,  431,\n",
      "        432,  433,  435,  436,  437,  438,  439,  440,  441,  442,  443,\n",
      "        445,  447,  448,  449,  450,  451,  452,  453,  454,  455,  456,\n",
      "        457,  458,  459,  461,  462,  464,  465,  467,  468,  469,  470,\n",
      "        471,  472,  473,  474,  475,  478,  479,  480,  481,  482,  483,\n",
      "        485,  486,  488,  489,  490,  491,  492,  494,  495,  496,  497,\n",
      "        498,  499,  500,  501,  502,  504,  505,  506,  507,  508,  509,\n",
      "        511,  512,  513,  514,  515,  516,  517,  518,  520,  521,  524,\n",
      "        526,  528,  529,  530,  531,  533,  534,  535,  537,  538,  539,\n",
      "        540,  541,  542,  543,  544,  546,  548,  550,  551,  552,  553,\n",
      "        554,  555,  556,  557,  560,  561,  562,  563,  564,  565,  566,\n",
      "        567,  569,  570,  571,  572,  573,  575,  576,  577,  580,  581,\n",
      "        583,  584,  585,  586,  588,  589,  590,  591,  593,  594,  595,\n",
      "        597,  598,  599,  601,  602,  604,  605,  606,  608,  609,  610,\n",
      "        611,  612,  613,  614,  615,  616,  618,  619,  620,  621,  622,\n",
      "        623,  624,  627,  628,  629,  630,  631,  632,  634,  635,  636,\n",
      "        637,  638,  639,  640,  642,  643,  644,  647,  648,  651,  652,\n",
      "        653,  654,  656,  658,  659,  660,  661,  662,  663,  664,  665,\n",
      "        668,  669,  670,  672,  673,  674,  675,  676,  677,  679,  680,\n",
      "        681,  682,  683,  684,  687,  688,  689,  691,  692,  693,  695,\n",
      "        696,  697,  698,  699,  700,  701,  702,  703,  704,  705,  706,\n",
      "        707,  708,  709,  710,  711,  713,  715,  716,  717,  718,  720,\n",
      "        721,  722,  723,  724,  726,  727,  728,  729,  730,  731,  732,\n",
      "        733,  734,  735,  736,  738,  739,  740,  741,  742,  744,  745,\n",
      "        746,  748,  750,  751,  752,  753,  754,  755,  756,  757,  758,\n",
      "        759,  760,  761,  762,  763,  764,  765,  766,  769,  770,  771,\n",
      "        773,  774,  775,  776,  777,  778,  779,  780,  782,  783,  784,\n",
      "        785,  786,  787,  790,  791,  792,  793,  794,  795,  796,  797,\n",
      "        799,  801,  802,  803,  804,  807,  808,  809,  810,  811,  812,\n",
      "        813,  814,  815,  817,  818,  821,  822,  823,  824,  826,  827,\n",
      "        828,  829,  831,  832,  833,  834,  835,  836,  840,  841,  842,\n",
      "        843,  844,  845,  846,  847,  848,  849,  850,  851,  852,  856,\n",
      "        857,  858,  859,  862,  863,  864,  865,  866,  867,  868,  869,\n",
      "        870,  871,  873,  874,  875,  876,  877,  878,  879,  880,  881,\n",
      "        882,  883,  884,  886,  887,  889,  890,  891,  892,  893,  896,\n",
      "        899,  900,  902,  903,  904,  905,  908,  909,  910,  911,  912,\n",
      "        913,  914,  915,  916,  917,  918,  919,  920,  921,  922,  923,\n",
      "        924,  925,  926,  927,  928,  929,  930,  931,  933,  935,  936,\n",
      "        937,  938,  939,  940,  941,  942,  943,  944,  945,  946,  948,\n",
      "        949,  950,  951,  952,  955,  958,  959,  960,  961,  962,  963,\n",
      "        965,  966,  967,  969,  970,  971,  972,  973,  974,  975,  976,\n",
      "        978,  980,  981,  984,  985,  986,  987,  988,  989,  990,  991,\n",
      "        993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n",
      "       1004, 1006, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016,\n",
      "       1017, 1018, 1019, 1020, 1022, 1023, 1024, 1025, 1026, 1027, 1028,\n",
      "       1029, 1031, 1032, 1033, 1034, 1036, 1037, 1038, 1039, 1040, 1041,\n",
      "       1042, 1044, 1045, 1046, 1047, 1048, 1052, 1053, 1054, 1057, 1059,\n",
      "       1060, 1061, 1062, 1063, 1065, 1066, 1068, 1069, 1070, 1073, 1074,\n",
      "       1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085,\n",
      "       1086, 1087, 1088, 1089, 1090, 1091, 1093, 1095, 1097, 1099, 1100,\n",
      "       1101, 1103, 1104, 1105, 1106, 1108, 1109, 1110, 1111, 1112, 1113,\n",
      "       1114, 1115, 1117, 1118, 1119, 1121, 1123, 1124, 1125, 1126, 1127,\n",
      "       1128, 1130, 1131, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140,\n",
      "       1141, 1142, 1145, 1146, 1147, 1148, 1149, 1150, 1152, 1154, 1156,\n",
      "       1157, 1158, 1159, 1160, 1161, 1162, 1163, 1165], dtype=int64), array([   3,    4,    7,   10,   28,   29,   35,   40,   46,   48,   52,\n",
      "         55,   62,   64,   70,   72,   73,   78,   83,   89,   95,  105,\n",
      "        114,  116,  118,  122,  127,  129,  135,  143,  144,  145,  146,\n",
      "        147,  156,  160,  167,  169,  170,  175,  179,  181,  185,  192,\n",
      "        207,  209,  221,  226,  233,  237,  238,  242,  245,  248,  251,\n",
      "        255,  257,  266,  269,  283,  285,  304,  307,  308,  311,  323,\n",
      "        331,  332,  337,  340,  343,  344,  351,  354,  367,  381,  383,\n",
      "        391,  392,  396,  399,  400,  402,  404,  408,  409,  415,  419,\n",
      "        420,  423,  429,  434,  444,  446,  460,  463,  466,  476,  477,\n",
      "        484,  487,  493,  503,  510,  519,  522,  523,  525,  527,  532,\n",
      "        536,  545,  547,  549,  558,  559,  568,  574,  578,  579,  582,\n",
      "        587,  592,  596,  600,  603,  607,  617,  625,  626,  633,  641,\n",
      "        645,  646,  649,  650,  655,  657,  666,  667,  671,  678,  685,\n",
      "        686,  690,  694,  712,  714,  719,  725,  737,  743,  747,  749,\n",
      "        767,  768,  772,  781,  788,  789,  798,  800,  805,  806,  816,\n",
      "        819,  820,  825,  830,  837,  838,  839,  853,  854,  855,  860,\n",
      "        861,  872,  885,  888,  894,  895,  897,  898,  901,  906,  907,\n",
      "        932,  934,  947,  953,  954,  956,  957,  964,  968,  977,  979,\n",
      "        982,  983,  992, 1005, 1007, 1021, 1030, 1035, 1043, 1049, 1050,\n",
      "       1051, 1055, 1056, 1058, 1064, 1067, 1071, 1072, 1092, 1094, 1096,\n",
      "       1098, 1102, 1107, 1116, 1120, 1122, 1129, 1132, 1143, 1144, 1151,\n",
      "       1153, 1155, 1164], dtype=int64))\n",
      "(array([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
      "         12,   14,   15,   16,   18,   19,   20,   21,   24,   25,   27,\n",
      "         28,   29,   30,   32,   33,   35,   36,   37,   40,   41,   42,\n",
      "         43,   46,   48,   50,   51,   52,   53,   54,   55,   56,   57,\n",
      "         59,   62,   63,   64,   65,   66,   67,   69,   70,   71,   72,\n",
      "         73,   74,   75,   76,   77,   78,   79,   80,   81,   82,   83,\n",
      "         84,   85,   86,   87,   88,   89,   90,   91,   92,   94,   95,\n",
      "         96,   97,   99,  101,  102,  103,  104,  105,  106,  107,  108,\n",
      "        109,  110,  111,  112,  113,  114,  115,  116,  117,  118,  119,\n",
      "        120,  121,  122,  123,  124,  125,  127,  128,  129,  130,  131,\n",
      "        133,  134,  135,  136,  137,  138,  139,  140,  141,  143,  144,\n",
      "        145,  146,  147,  150,  151,  152,  153,  154,  156,  157,  159,\n",
      "        160,  161,  163,  165,  166,  167,  168,  169,  170,  171,  173,\n",
      "        174,  175,  178,  179,  180,  181,  182,  185,  186,  187,  188,\n",
      "        189,  190,  191,  192,  194,  196,  197,  200,  201,  202,  203,\n",
      "        204,  205,  206,  207,  208,  209,  210,  211,  213,  214,  215,\n",
      "        218,  219,  221,  222,  223,  224,  225,  226,  227,  229,  230,\n",
      "        231,  232,  233,  234,  236,  237,  238,  239,  240,  241,  242,\n",
      "        243,  244,  245,  246,  248,  249,  250,  251,  252,  254,  255,\n",
      "        257,  258,  259,  260,  261,  262,  263,  264,  265,  266,  268,\n",
      "        269,  270,  271,  272,  273,  274,  275,  276,  277,  279,  280,\n",
      "        281,  282,  283,  285,  286,  287,  288,  289,  290,  292,  293,\n",
      "        294,  295,  296,  297,  298,  300,  301,  302,  303,  304,  307,\n",
      "        308,  310,  311,  312,  314,  315,  316,  317,  319,  320,  321,\n",
      "        322,  323,  324,  325,  326,  327,  328,  329,  330,  331,  332,\n",
      "        333,  334,  335,  336,  337,  338,  340,  341,  342,  343,  344,\n",
      "        345,  346,  347,  348,  350,  351,  352,  353,  354,  355,  356,\n",
      "        358,  359,  360,  363,  365,  366,  367,  368,  370,  371,  372,\n",
      "        373,  374,  377,  379,  381,  383,  384,  385,  386,  387,  388,\n",
      "        389,  390,  391,  392,  393,  394,  396,  397,  398,  399,  400,\n",
      "        401,  402,  404,  406,  407,  408,  409,  410,  411,  412,  413,\n",
      "        414,  415,  416,  417,  419,  420,  422,  423,  424,  425,  426,\n",
      "        427,  428,  429,  430,  431,  432,  433,  434,  435,  436,  437,\n",
      "        438,  440,  442,  444,  445,  446,  448,  449,  450,  451,  452,\n",
      "        453,  455,  456,  457,  459,  460,  461,  462,  463,  464,  465,\n",
      "        466,  467,  468,  469,  470,  471,  473,  475,  476,  477,  478,\n",
      "        480,  481,  482,  483,  484,  485,  486,  487,  488,  489,  490,\n",
      "        491,  493,  495,  496,  497,  498,  499,  500,  502,  503,  504,\n",
      "        505,  507,  509,  510,  511,  512,  515,  517,  518,  519,  520,\n",
      "        521,  522,  523,  524,  525,  527,  529,  531,  532,  534,  535,\n",
      "        536,  537,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
      "        548,  549,  550,  552,  554,  555,  557,  558,  559,  560,  561,\n",
      "        562,  563,  564,  565,  567,  568,  570,  571,  572,  573,  574,\n",
      "        575,  577,  578,  579,  580,  581,  582,  583,  584,  585,  586,\n",
      "        587,  588,  589,  590,  591,  592,  593,  594,  595,  596,  597,\n",
      "        598,  599,  600,  601,  602,  603,  604,  605,  606,  607,  608,\n",
      "        609,  612,  613,  614,  615,  617,  619,  622,  623,  624,  625,\n",
      "        626,  627,  628,  630,  631,  632,  633,  634,  635,  636,  638,\n",
      "        640,  641,  644,  645,  646,  647,  648,  649,  650,  651,  652,\n",
      "        653,  655,  656,  657,  658,  659,  660,  661,  662,  663,  665,\n",
      "        666,  667,  669,  670,  671,  672,  673,  674,  675,  676,  678,\n",
      "        679,  680,  682,  685,  686,  687,  688,  689,  690,  691,  692,\n",
      "        694,  695,  696,  697,  698,  699,  700,  703,  705,  706,  708,\n",
      "        709,  711,  712,  713,  714,  715,  716,  717,  718,  719,  720,\n",
      "        721,  723,  724,  725,  726,  727,  728,  729,  731,  732,  735,\n",
      "        736,  737,  740,  741,  742,  743,  745,  746,  747,  748,  749,\n",
      "        751,  752,  753,  754,  755,  756,  757,  758,  759,  760,  761,\n",
      "        764,  766,  767,  768,  769,  770,  771,  772,  773,  774,  775,\n",
      "        777,  778,  779,  780,  781,  782,  784,  785,  786,  787,  788,\n",
      "        789,  790,  791,  792,  793,  794,  796,  797,  798,  800,  801,\n",
      "        802,  803,  805,  806,  808,  809,  810,  811,  812,  813,  815,\n",
      "        816,  817,  819,  820,  821,  822,  823,  824,  825,  827,  828,\n",
      "        830,  831,  832,  833,  834,  837,  838,  839,  840,  841,  842,\n",
      "        843,  844,  846,  847,  848,  849,  850,  851,  852,  853,  854,\n",
      "        855,  857,  858,  860,  861,  862,  863,  864,  866,  868,  869,\n",
      "        870,  871,  872,  874,  875,  877,  878,  881,  884,  885,  888,\n",
      "        889,  890,  891,  893,  894,  895,  897,  898,  900,  901,  904,\n",
      "        905,  906,  907,  908,  909,  910,  912,  914,  915,  916,  917,\n",
      "        918,  920,  922,  924,  925,  926,  927,  928,  929,  930,  931,\n",
      "        932,  934,  935,  936,  940,  941,  942,  943,  944,  946,  947,\n",
      "        948,  951,  952,  953,  954,  955,  956,  957,  959,  960,  962,\n",
      "        964,  967,  968,  969,  970,  971,  972,  973,  975,  976,  977,\n",
      "        979,  980,  981,  982,  983,  984,  985,  987,  988,  989,  990,\n",
      "        991,  992,  993,  994,  996,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1007, 1009, 1010, 1011, 1013, 1014, 1016, 1018, 1019, 1021,\n",
      "       1022, 1023, 1025, 1026, 1028, 1029, 1030, 1032, 1033, 1035, 1036,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1045, 1046, 1048, 1049, 1050,\n",
      "       1051, 1053, 1054, 1055, 1056, 1057, 1058, 1061, 1062, 1064, 1065,\n",
      "       1066, 1067, 1068, 1069, 1071, 1072, 1073, 1074, 1076, 1078, 1079,\n",
      "       1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1089, 1090, 1091,\n",
      "       1092, 1093, 1094, 1096, 1097, 1098, 1099, 1101, 1102, 1103, 1105,\n",
      "       1106, 1107, 1108, 1109, 1110, 1114, 1115, 1116, 1117, 1119, 1120,\n",
      "       1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131,\n",
      "       1132, 1133, 1135, 1137, 1139, 1140, 1142, 1143, 1144, 1145, 1146,\n",
      "       1147, 1148, 1150, 1151, 1152, 1153, 1154, 1155, 1157, 1158, 1159,\n",
      "       1160, 1164], dtype=int64), array([   0,   13,   17,   22,   23,   26,   31,   34,   38,   39,   44,\n",
      "         45,   47,   49,   58,   60,   61,   68,   93,   98,  100,  126,\n",
      "        132,  142,  148,  149,  155,  158,  162,  164,  172,  176,  177,\n",
      "        183,  184,  193,  195,  198,  199,  212,  216,  217,  220,  228,\n",
      "        235,  247,  253,  256,  267,  278,  284,  291,  299,  305,  306,\n",
      "        309,  313,  318,  339,  349,  357,  361,  362,  364,  369,  375,\n",
      "        376,  378,  380,  382,  395,  403,  405,  418,  421,  439,  441,\n",
      "        443,  447,  454,  458,  472,  474,  479,  492,  494,  501,  506,\n",
      "        508,  513,  514,  516,  526,  528,  530,  533,  538,  551,  553,\n",
      "        556,  566,  569,  576,  610,  611,  616,  618,  620,  621,  629,\n",
      "        637,  639,  642,  643,  654,  664,  668,  677,  681,  683,  684,\n",
      "        693,  701,  702,  704,  707,  710,  722,  730,  733,  734,  738,\n",
      "        739,  744,  750,  762,  763,  765,  776,  783,  795,  799,  804,\n",
      "        807,  814,  818,  826,  829,  835,  836,  845,  856,  859,  865,\n",
      "        867,  873,  876,  879,  880,  882,  883,  886,  887,  892,  896,\n",
      "        899,  902,  903,  911,  913,  919,  921,  923,  933,  937,  938,\n",
      "        939,  945,  949,  950,  958,  961,  963,  965,  966,  974,  978,\n",
      "        986,  995,  997,  998, 1006, 1008, 1012, 1015, 1017, 1020, 1024,\n",
      "       1027, 1031, 1034, 1037, 1044, 1047, 1052, 1059, 1060, 1063, 1070,\n",
      "       1075, 1077, 1088, 1095, 1100, 1104, 1111, 1112, 1113, 1118, 1134,\n",
      "       1136, 1138, 1141, 1149, 1156, 1161, 1162, 1163, 1165], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "house_prices = data.HousePrices()\n",
    "classifier = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_split=2, max_features = None)\n",
    "report = k_fold_cv(classifier,5, house_prices.X_train, house_prices.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c9e6854961fc04cc57075be415c2921",
     "grade": false,
     "grade_id": "q34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 3.4 [4 points] What is the best `max_depth` (consider depths from 1 to 10) and best cross validation accuracy chosen from 5-fold cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa2f1e4417169b3f8ce01f1ddd577f00",
     "grade": true,
     "grade_id": "a34",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(4)  # changing the seed might yield different results\n",
    "best_depth, best_accuracy = -1, 0\n",
    "\n",
    "#Workspace 3.4\n",
    "#TODO: \n",
    "#BEGIN \n",
    "# code here\n",
    "#END\n",
    "print(\"Cross validation accuracy for chosen best max_depth %d: %f\" % (best_depth, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b6c43c70dd992dd0452c58b97ef07aa",
     "grade": false,
     "grade_id": "q35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 3.5 [2 pts] report the accuracy of the model with the best `max_depth` on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80f577dd7355453cd0e1e2adae377d07",
     "grade": true,
     "grade_id": "a35",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_accuracy = 0\n",
    "#Workspace 3.5\n",
    "#BEGIN \n",
    "# code here\n",
    "#END\n",
    "print (\"accuracy of the best model on the testing set\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f48d0be5f104e783aafd3b0a9076d51",
     "grade": false,
     "grade_id": "p4_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 4 - Decision Tree Ensembles: Bagging [38 pts] and BONUS Boosting [5 pts]\n",
    "---\n",
    "We've seen that a DecisionTreeClassifier with depth = 3 is far from being the best performing on our house prices data.\n",
    "\n",
    "In this problem, we will introduce 3 ensemble methods to _boost_ the performance of this poor and underestimated weak learner.\n",
    "\n",
    "Whenever we need to generate a new instance of our weak learner, we'll have to call `get_weak_leaner`.\n",
    "You can see below that the weak learner achieves lower accuracy compared to the tree from the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33ca93589b8aa21fed36aff8be61a28d",
     "grade": false,
     "grade_id": "weak_learner",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_weak_learner():\n",
    "    \"\"\"Return a new instance of out chosen weak learner\"\"\"\n",
    "    return DecisionTreeClassifier(max_depth=3, min_samples_leaf=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weak_clf = get_weak_learner().fit(house_prices.X_train, house_prices.y_train)\n",
    "print(\"Accuracy on the training set:\", weak_clf.score(house_prices.X_train,house_prices.y_train))\n",
    "print(\"Accuracy on the test set:    \", weak_clf.score(house_prices.X_test,house_prices.y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df8b4f8dee15fa977eeaf3984e4af332",
     "grade": false,
     "grade_id": "q41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Start by completing the `EnsembleTest` class that we'll use to evaluate different ensemble methods.\n",
    "\n",
    "- 4.1 [5 points] Complete the `ensemble_test` class to fit the model received as parameter and store the metrics and running time.\n",
    "\n",
    "You can use the function `plot_metric` to show and compare different statistics of each model in a bar chart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f473026674011dcc7c9b0219e8033356",
     "grade": true,
     "grade_id": "a41",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import precision_score\n",
    "import pandas as pd\n",
    "\n",
    "class EnsembleTest:\n",
    "    \"\"\"\n",
    "        Test multiple model performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        initialize EnsembleTest\n",
    "        :param data: dataset containing Training and Test sets\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.execution_time = {} # dictionary with key: model name, value: time taken to fit and score the model\n",
    "        self.metric = {} # dictionary with key: model name, value: accuracy\n",
    "        self.scores = {}# dictionary with key: model name, value: weighted average precision\n",
    "        self.score_name = 'Precision(weighted)'\n",
    "        self.metric_name = 'Mean accuracy'\n",
    "\n",
    "    def evaluate_model(self, model, name):\n",
    "        \"\"\"\n",
    "        Fit the model using the training data and save the evaluations metrics on the test set\n",
    "        :param model: classifier to evaluate\n",
    "        :param name: name of model\n",
    "        \"\"\"\n",
    "        start = time()\n",
    "        #Workspace 4.1\n",
    "        #TODO: Fit the model and get the predictions to compute the metric and the score\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        self.execution_time[name] = time() - start\n",
    "\n",
    "    def print_result(self):\n",
    "        \"\"\"\n",
    "            print results for all models trained and tested.\n",
    "        \"\"\"\n",
    "        models_cross = pd.DataFrame({\n",
    "            'Model': list(self.metric.keys()),\n",
    "            self.score_name: list(self.scores.values()),\n",
    "            self.metric_name: list(self.metric.values()),\n",
    "            'Execution time': list(self.execution_time.values())})\n",
    "        print(models_cross.sort_values(by=self.score_name, ascending=False))\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"\n",
    "        Plot bar chart, one for each statistic (metric, score, running time)\n",
    "        \"\"\"\n",
    "        fig, axs = plt.subplots(1, 3)\n",
    "        fig.set_figheight(6), fig.set_figwidth(18)\n",
    "        p = 0\n",
    "        for stats, name in zip([self.metric, self.scores, self.execution_time],\n",
    "                               [self.metric_name, self.score_name, \"Elapsed time\"]):\n",
    "            left = [i for i in range(len(stats))]\n",
    "            height = [stats[key] for key in stats]\n",
    "            tick_label = [key for key in stats]\n",
    "            axs[p].set_title(name)\n",
    "            axs[p].bar(left, height, tick_label=tick_label, width=0.5)\n",
    "            p += 1\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd0a5e27566aee001183dbf207233814",
     "grade": false,
     "grade_id": "q42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 4.2 [3 points] Test `EnsembleTest` using our weak learner returned by `get_weak_learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be0cb7020c5ced8d49832d8d5e4ffaf4",
     "grade": true,
     "grade_id": "a42",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(house_prices)\n",
    "#Workspace 4.2\n",
    "#TODO: Initialize weak learner and evaluate it using evaluate_model\n",
    "#BEGIN \n",
    "# code here\n",
    "#END\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a07635788e6f05c0049b2a9a047d7c7",
     "grade": false,
     "grade_id": "qbagging",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Bagging:**\n",
    "\n",
    "The first Ensemble technique we deal with is called _Bagging_ (Bootstrap AGGregatING).\n",
    "Bagging consists of training a number of weak learners using randomly sampled instances from our data (with replacement). We have to start\n",
    "by choosing the number of estimators we want to use. Then for each estimator, we sample a random subset of the data to fit the estimator.\n",
    "\n",
    "To compute the prediction, we sum the prediction probabilities of the estimators and return the label that has the highest\n",
    "accumulated probabilities.\n",
    "\n",
    "- 4.3 [5 points] First, complete `sample_data` to return a random sample of size `sample_ratio* len(X_train)` of features and labels\n",
    "\n",
    "- 4.4 [5 points] Complete `fit` by instantiating `n_estimators` of our weak leaner, each trained on random sample of the data\n",
    "\n",
    "- 4.5 [5 points] Complete `predict` method to return the most likely label by combining different estimators predictions.\n",
    "Instead of the majority vote used in KNNClassifier, you should use `predict_proba` method of DecisionTreeClassifier.\n",
    "[See Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42659a7181d9a33972dc1cf43ba70c92",
     "grade": true,
     "grade_id": "abagging",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaggingEnsemble(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Initialize BaggingEnsemble\n",
    "        :param n_estimators: number of estimators/weak learner to use\n",
    "        :param sample_ratio: ratio of the training data to sample\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.estimators = [] # List used in fit method to store the trained estimators\n",
    "\n",
    "    def sample_data(self, X_train, y_train):\n",
    "        X_sample, y_sample = None, None\n",
    "        #Workspace 4.3\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train), sampling is with replacement (iid)\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return X_sample, y_sample\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train the different estimators on sampled data using provided training samples\n",
    "        :param X_train: training samples, shape (num_samples, num_features)\n",
    "        :param y_train: training labels, shape (num_samples)\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        np.random.seed(42) # Keep it to get consistent results across runs, you can change the seed value\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            #Workspace 4.4\n",
    "            #BEGIN \n",
    "            # code here\n",
    "            #END\n",
    "        return self\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the labels of test samples\n",
    "        :param X_test: array of shape (num_points, num_features)\n",
    "        :return: 1-d array of shape (num_points)\n",
    "        \"\"\"\n",
    "        predicted_proba = 0\n",
    "        answer = 0\n",
    "        #Workspace 4.5\n",
    "        #TODO: go through the trained estimators and accumulate their predicted_proba to get the mostly likely label\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.evaluate_model(BaggingEnsemble(10, 0.9), 'Bagging')\n",
    "ensemble_handler.print_result()\n",
    "ensemble_handler.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73c559db55e7a1ed5dead72ea9280663",
     "grade": false,
     "grade_id": "qforest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Random Forest**\n",
    "\n",
    "Random Forest has an additional layer of randomness compared to Bagging: we also sample a random subset of the features (columns).\n",
    "The rest of the implementation should be similar if not exactly the same as Bagging. In addition to keeping track of the estimators \n",
    "(in `RandomForest.estimators`, we also have to store the features indices that are used by each estimator (in `RandomForest.features_indices`).\n",
    "\n",
    "\n",
    "- 4.6 [4 points] First, complete `sample_data` to return a random sample of size `sample_ratio* len(X_train)` of labels and `feature_ratio * num_features` of features\n",
    "\n",
    "- 4.7 [4 points] Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data.\n",
    "Make sure to keep track of the sampled features for each estimator to use them in the prediction step\n",
    "\n",
    "- 4.8 [4 points] Complete `predict` method to return the most likely label by combining different estimators predictions. Instead of the majority vote used in KNNClassifier, you should use `predict_proba` method DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cd630a2ee19c63df7aab121c7504c5e",
     "grade": true,
     "grade_id": "aforest",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio=1.0, features_ratio=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.features_ratio = features_ratio\n",
    "        self.estimators = [] # to store the estimator\n",
    "        self.features_indices = [] # to store the feature indices used by each estimator\n",
    "\n",
    "    def sample_data(self, X_train, y_train):\n",
    "        X_sample, y_sample, features_indices = None, None, None\n",
    "        #Workspace 4.6\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train) and subset of features of size\n",
    "        #         features_ratio * num_features\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return X_sample, y_sample, features_indices\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        np.random.seed(42) # keep to have consistent results across run, you can change the value\n",
    "        for _ in range(self.n_estimators):\n",
    "            #Workspace 4.7\n",
    "            #TODO: sample data with random subset of rows and features using sample_data\n",
    "            #Hint: keep track of the features indices in features_indices to use in predict\n",
    "            #BEGIN \n",
    "            # code here\n",
    "            #END\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = 0\n",
    "        answer = 0\n",
    "        #Workspace 4.8\n",
    "        #TODO: compute cumulative sum of predict proba from estimators and return the labels with highest likelihood\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.evaluate_model(RandomForest(200, sample_ratio=0.7, features_ratio=0.1), 'RandomForest')\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "584dccdf58532a00220a3536805d006f",
     "grade": false,
     "grade_id": "q49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- 4.9 [3 points] Add different ensemble methods to the handler (try different parameters), plot, show, and compare them.\n",
    "What's the best weighted average precision we can get? What's the best accuracy? Which ensemble method achieves each of them?\n",
    "You can also compare to our best decision tree found in Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4e3888c7d6bd4be0cd43d1e0e928ea9",
     "grade": true,
     "grade_id": "a49a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(house_prices)\n",
    "ensemble_handler.evaluate_model(get_weak_learner(), 'weak_learner')\n",
    "#Workspace 4.9.a\n",
    "#TODO Add multiple instances of the ensemble methods. Plot and compare their performance\n",
    "#You can also add best tree from problem 3\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b73965b0877e99e50e36b4b6f7c7d87",
     "grade": true,
     "grade_id": "a49b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Write-up 4.9.b\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b45a7de03f8aa13a40119df78111a288",
     "grade": false,
     "grade_id": "qboosting",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**BONUS - Boosting**\n",
    "\n",
    "There are different methods of boosting, but we'll focus in this problem on Adaptive Boosting (AdaBoost).\n",
    "The logic of AdaBoost is to \"push\" each new learner to give more importance to previously misclassified data. We present\n",
    "below the multiclass variant of AdaBoost [SAMME](https://web.stanford.edu/~hastie/Papers/samme.pdf). We denote $K$ the number of classes.\n",
    "\n",
    "AdaBosst is performed by increasing the weights of misclassified samples after each iteration:\n",
    "- Input: m samples $(X_i, y_i)_{i\\in [m]}$, number of boosting rounds $N$\n",
    "- Start with equal samples weights $W = (w_i), $ where   $w_i = \\frac{1}{\\texttt{n_samples}}$\n",
    "- at round j:\n",
    "    - Train estimator $h_j$ using current weights $W$\n",
    "    - Get the predicted $(\\hat{y}_i)$ on the training data using $h_j$\n",
    "    - Find the weighted error rate $\\epsilon_j$ using $W$: $\\epsilon_j=\\frac{\\sum_i w_i \\Delta(\\hat{y}_i, y_i)}{\\sum_i w_i}$\n",
    "    - Choose $\\alpha_j = \\log \\frac{1-\\epsilon_j}{\\epsilon_j} + \\log(K-1)$\n",
    "    - Update $W$ using: $w_i \\leftarrow w_i \\exp(\\alpha_j \\Delta(\\hat{y_i}, y_i)) $\n",
    "    - Normalize $W$ to have sum 1\n",
    "- Global estimator is $H = \\sum_j \\alpha_j h_j$,\n",
    "\n",
    "the $\\Delta$ function equals to 1 when the two argument are different, 0 otherwise.\n",
    "\n",
    "To understand how we implement $H$, imaging we have two classes, and we boosted for 3 rounds to get $(h_1, h_2, h_3)$,\n",
    "with weights $(\\alpha_1, \\alpha_2, \\alpha_3)$. When we want to predict the label of sample $x$, we get $(h_1(x), h_2(x), h_3(x)) = (0,1,0)$.\n",
    "\n",
    "In this case, label $0$ gets a weight $\\alpha_1+\\alpha_2$, while class $1$ get weight $\\alpha_2$. The predicted class is the one with\n",
    "the largest weight (1 if $\\alpha_2 > \\alpha_1 + \\alpha_3$, 0 otherwise)\n",
    "\n",
    "\n",
    "- **(Bonus)** 4.10 [4 pts] Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on the same data but with different samples weights as detailed in the algorithm. Keep track of $(\\alpha_i)$\n",
    "\n",
    "_Hint: our weak learner (DecisionTreeClassifier) can take an argument `sample_weight` when calling the `fit` method, you'll have to use it to provide the weights $W$_\n",
    " \n",
    "\n",
    "The `predict` method function is provided which returns the predicted label using the global estimator $H$. It uses one hot encoding of the predicted labels from the weak learners and cumulate the prediction with weights $\\alpha_j$. \n",
    "\n",
    "Notice that if the estimator is consistent (0 error rate on the training set), AdaBoost $\\alpha_j$ are no longer defined. That's why this method requires a **weak** learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96fa2985248204998028bcd851bea847",
     "grade": true,
     "grade_id": "aboosting",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoost(object):\n",
    "\n",
    "    def __init__(self, n_estimators):\n",
    "        \"\"\"\n",
    "        :param n_estimators: number of estimators/ boosting rounds\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.num_classes = None\n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        self.num_classes = np.unique(y_train).shape[0] # K in the algorithm\n",
    "        weights = np.ones(len(X_train)) / len(X_train) # W in the algorithm\n",
    "        # Workspace 4.10\n",
    "        #TODO: Implement Multiclass Adaboost and keep track of the alpha_j\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        #END\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        get the labels returned by the global estimator defined as H\n",
    "        the predicted label is the one that accumulates the largest sum of alphas\n",
    "        '''\n",
    "        # y_hat is one-hot encoding of the multi-class labels\n",
    "        y_hat = np.zeros((X_test.shape[0], self.num_classes))\n",
    "        for i in range(len(self.estimators)):\n",
    "            # np.eye(k)[i] returns the one-hot encoding of size k for label i\n",
    "            # np.eye(4)[2] would be [0, 0, 1, 0], np.eye(4)[0] is [1,0, 0, 0]. Clever, innit?\n",
    "            y_hat += self.alphas[i] * np.eye(self.num_classes)[self.estimators[i].predict(X_test)]\n",
    "        answer = np.argmax(y_hat, axis=1)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_handler.evaluate_model(RandomForest(100, sample_ratio=0.8, features_ratio=0.8), 'RandomForest')\n",
    "ensemble_handler.evaluate_model(AdaBoost(40), 'AdaBoost')\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8506fe6cca0537e16db2e09037fc6035",
     "grade": false,
     "grade_id": "q411",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **Comparison**\n",
    "\n",
    "- **(Bonus)**  4.11 [1 point] Run the same comparison as in 4.9 including AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf66239070baceb8bf6475b0b1ad7fc",
     "grade": true,
     "grade_id": "a411a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(house_prices)\n",
    "ensemble_handler.evaluate_model(get_weak_learner(), 'weak_learner')\n",
    "#Workspace 4.11.a\n",
    "#TODO Add multiple instances of the ensemble methods. Plot and compare their performance\n",
    "#You can also add best tree from problem 3\n",
    "#BEGIN \n",
    "# code here\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4c99b2c2328356a7e4b6068407a02a6",
     "grade": true,
     "grade_id": "a411b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Write-up 4.11.b\n",
    "%BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "%END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
